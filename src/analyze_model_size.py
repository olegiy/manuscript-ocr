"""
–î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ä–∞–∑–º–µ—Ä–∞ –º–æ–¥–µ–ª–∏ TRBA –ø–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–∞–º.
–ü–æ–∫–∞–∑—ã–≤–∞–µ—Ç, —Å–∫–æ–ª—å–∫–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –∏ –º–µ–≥–∞–±–∞–π—Ç –∑–∞–Ω–∏–º–∞–µ—Ç –∫–∞–∂–¥–∞—è —á–∞—Å—Ç—å.
"""

import sys
import os

# –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Ç—å –∫ src
sys.path.insert(0, r"C:\Users\USER\manuscript-ocr\src")

import torch

# –ü—Ä—è–º–æ–π –∏–º–ø–æ—Ä—Ç –±–µ–∑ –ø–∞–∫–µ—Ç–∞ manuscript
from manuscript.recognizers._trba.model.model import TRBAModel

# –ü—Ä–æ—Å—Ç–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ charset –±–µ–∑ –∏–º–ø–æ—Ä—Ç–∞ transforms
def load_charset(charset_path):
    with open(charset_path, 'r', encoding='utf-8') as f:
        chars = [line.strip() for line in f if line.strip()]
    itos = chars
    stoi = {char: idx for idx, char in enumerate(chars)}
    return itos, stoi

CHARSET_PATH = r"C:\Users\USER\manuscript-ocr\src\manuscript\recognizers\_trba\configs\charset.txt"

# –ó–∞–≥—Ä—É–∑–∫–∞ charset
itos, stoi = load_charset(CHARSET_PATH)
num_classes = len(itos)

print("=" * 80)
print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –†–ê–ó–ú–ï–†–ê –ú–û–î–ï–õ–ò TRBA")
print("=" * 80)
print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
print()

# –°–æ–∑–¥–∞—ë–º —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—É—é –º–æ–¥–µ–ª—å (2√ó256)
model = TRBAModel(
    num_classes=num_classes,
    hidden_size=256,
    num_encoder_layers=2,
    img_h=64,
    img_w=256,
    cnn_in_channels=3,
    cnn_out_channels=512,
    sos_id=stoi["<SOS>"],
    eos_id=stoi["<EOS>"],
    pad_id=stoi["<PAD>"],
    blank_id=stoi.get("<BLANK>", None),
)

def count_parameters(module):
    """–ü–æ–¥—Å—á—ë—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –º–æ–¥—É–ª—è"""
    return sum(p.numel() for p in module.parameters())

def format_size(num_params):
    """–§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –≤ MB (float32)"""
    mb = num_params * 4 / (1024 * 1024)  # 4 –±–∞–π—Ç–∞ –Ω–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä (float32)
    return mb

def analyze_module(name, module, indent=0):
    """–†–µ–∫—É—Ä—Å–∏–≤–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –º–æ–¥—É–ª—è"""
    params = count_parameters(module)
    mb = format_size(params)
    prefix = "  " * indent
    print(f"{prefix}{name:<40} {params:>12,} params  ({mb:>7.2f} MB)")
    return params, mb

print("=" * 80)
print("–û–ë–©–ê–Ø –°–¢–†–£–ö–¢–£–†–ê")
print("=" * 80)

total_params = count_parameters(model)
total_mb = format_size(total_params)

# –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –æ—Å–Ω–æ–≤–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
cnn_params, cnn_mb = analyze_module("1. CNN (SEResNet31)", model.cnn)
pool_params = 0  # AdaptiveAvgPool –Ω–µ –∏–º–µ–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
enc_rnn_params, enc_rnn_mb = analyze_module("2. Encoder RNN (BiLSTM)", model.enc_rnn)
attn_params, attn_mb = analyze_module("3. Attention Decoder", model.attn)

print("-" * 80)
print(f"{'–ò–¢–û–ì–û':<40} {total_params:>12,} params  ({total_mb:>7.2f} MB)")
print()

# –ü—Ä–æ—Ü–µ–Ω—Ç–Ω–æ–µ —Å–æ–æ—Ç–Ω–æ—à–µ–Ω–∏–µ
print("=" * 80)
print("–ü–†–û–¶–ï–ù–¢–ù–û–ï –°–û–û–¢–ù–û–®–ï–ù–ò–ï")
print("=" * 80)
print(f"CNN:              {cnn_params/total_params*100:>6.2f}%  ({cnn_mb:>7.2f} MB)")
print(f"Encoder RNN:      {enc_rnn_params/total_params*100:>6.2f}%  ({enc_rnn_mb:>7.2f} MB)")
print(f"Attention Decoder: {attn_params/total_params*100:>6.2f}%  ({attn_mb:>7.2f} MB)")
print()

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ CNN
print("=" * 80)
print("–î–ï–¢–ê–õ–ò CNN (SEResNet31)")
print("=" * 80)

# –ê–Ω–∞–ª–∏–∑ —Å–ª–æ—ë–≤ CNN
for name, module in model.cnn.named_children():
    params = count_parameters(module)
    mb = format_size(params)
    print(f"  {name:<38} {params:>12,} params  ({mb:>7.2f} MB)")

print()

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Encoder RNN
print("=" * 80)
print("–î–ï–¢–ê–õ–ò ENCODER RNN (BiLSTM)")
print("=" * 80)

for idx, layer in enumerate(model.enc_rnn):
    params = count_parameters(layer)
    mb = format_size(params)
    print(f"  BiLSTM Layer {idx+1:<28} {params:>12,} params  ({mb:>7.2f} MB)")
    
    # –î–µ—Ç–∞–ª–∏ BiLSTM —Å–ª–æ—è
    rnn_params = count_parameters(layer.rnn)
    linear_params = count_parameters(layer.linear)
    print(f"    ‚îú‚îÄ LSTM (bidirectional)        {rnn_params:>12,} params  ({format_size(rnn_params):>7.2f} MB)")
    print(f"    ‚îî‚îÄ Linear projection           {linear_params:>12,} params  ({format_size(linear_params):>7.2f} MB)")

print()

# –î–µ—Ç–∞–ª—å–Ω—ã–π –∞–Ω–∞–ª–∏–∑ Attention Decoder
print("=" * 80)
print("–î–ï–¢–ê–õ–ò ATTENTION DECODER")
print("=" * 80)

attn_cell_params = count_parameters(model.attn.attention_cell)
generator_params = count_parameters(model.attn.generator)

print(f"  Attention Cell                     {attn_cell_params:>12,} params  ({format_size(attn_cell_params):>7.2f} MB)")
print(f"  Generator (Linear)                 {generator_params:>12,} params  ({format_size(generator_params):>7.2f} MB)")

# –î–µ—Ç–∞–ª–∏ Attention Cell
print("\n  –î–µ—Ç–∞–ª–∏ Attention Cell:")
for name, module in model.attn.attention_cell.named_children():
    params = count_parameters(module)
    mb = format_size(params)
    print(f"    {name:<36} {params:>12,} params  ({mb:>7.2f} MB)")

print()

# –ê–Ω–∞–ª–∏–∑ –≤–ª–∏—è–Ω–∏—è –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
print("=" * 80)
print("–í–õ–ò–Ø–ù–ò–ï –ê–†–•–ò–¢–ï–ö–¢–£–†–ù–´–• –ü–ê–†–ê–ú–ï–¢–†–û–í –ù–ê –†–ê–ó–ú–ï–†")
print("=" * 80)

configs = [
    ("–ú–∏–∫—Ä–æ (1√ó128, CNN=256)", 1, 128, 256),
    ("–õ–µ–≥–∫–∞—è (1√ó256, CNN=512)", 1, 256, 512),
    ("–°—Ç–∞–Ω–¥–∞—Ä—Ç (2√ó256, CNN=512)", 2, 256, 512),
    ("–¢—è–∂–µ–ª–∞—è (2√ó512, CNN=512)", 2, 512, 512),
    ("–ú–æ—â–Ω–∞—è CNN (2√ó256, CNN=768)", 2, 256, 768),
    ("–ì–ª—É–±–æ–∫–∞—è (4√ó256, CNN=512)", 4, 256, 512),
]

print(f"{'–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è':<30} {'Params':>12} {'Size (MB)':>12} {'vs Baseline':>12}")
print("-" * 80)

baseline_params = None
for name, num_enc, hidden, cnn_out in configs:
    m = TRBAModel(
        num_classes=num_classes,
        hidden_size=hidden,
        num_encoder_layers=num_enc,
        img_h=64,
        img_w=256,
        cnn_in_channels=3,
        cnn_out_channels=cnn_out,
        sos_id=stoi["<SOS>"],
        eos_id=stoi["<EOS>"],
        pad_id=stoi["<PAD>"],
        blank_id=stoi.get("<BLANK>", None),
    )
    
    params = count_parameters(m)
    mb = format_size(params)
    
    if baseline_params is None:
        baseline_params = params
        ratio = "baseline"
    else:
        ratio = f"{params/baseline_params:.2f}√ó"
    
    print(f"{name:<30} {params:>12,} {mb:>11.2f} MB {ratio:>12}")

print()

# –ß—Ç–æ –º–æ–∂–Ω–æ —Å–æ–∫—Ä–∞—Ç–∏—Ç—å
print("=" * 80)
print("üîß –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –ü–û –£–ú–ï–ù–¨–®–ï–ù–ò–Æ –†–ê–ó–ú–ï–†–ê")
print("=" * 80)

print("""
1. CNN (SEResNet31) ‚Äî —Å–∞–º–∞—è —Ç—è–∂–µ–ª–∞—è —á–∞—Å—Ç—å (~60-70% –º–æ–¥–µ–ª–∏)
   - –£–º–µ–Ω—å—à–∏—Ç—å cnn_out_channels: 512 ‚Üí 384 –∏–ª–∏ 256 (-25-50%)
   - –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –±–æ–ª–µ–µ –ª–µ–≥–∫–∏–π backbone (MobileNet, EfficientNet)
   - –ü—Ä–∏–º–µ–Ω–∏—Ç—å –∫–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—é (INT8 –≤–º–µ—Å—Ç–æ FP32) ‚Üí √∑4 —Ä–∞–∑–º–µ—Ä

2. Encoder RNN (BiLSTM) ‚Äî 15-25% –º–æ–¥–µ–ª–∏
   - –£–º–µ–Ω—å—à–∏—Ç—å hidden_size: 256 ‚Üí 128 (-50% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
   - –£–º–µ–Ω—å—à–∏—Ç—å num_encoder_layers: 2 ‚Üí 1 (-50% –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤)
   - BiLSTM –∑–∞–º–µ–Ω–∏—Ç—å –Ω–∞ –æ–±—ã—á–Ω—ã–π LSTM (bidirectional=False) ‚Üí √∑2

3. Attention Decoder ‚Äî 10-20% –º–æ–¥–µ–ª–∏
   - –£–º–µ–Ω—å—à–∏—Ç—å hidden_size (–≤–ª–∏—è–µ—Ç –Ω–∞ attention_cell –∏ generator)
   - –£–ø—Ä–æ—Å—Ç–∏—Ç—å attention mechanism (–Ω–∞–ø—Ä–∏–º–µ—Ä, dot-product –≤–º–µ—Å—Ç–æ MLP)

4. –û–±—â–∏–µ –º–µ—Ç–æ–¥—ã
   - –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è (FP32 ‚Üí INT8): √∑4 —Ä–∞–∑–º–µ—Ä, –Ω–µ–±–æ–ª—å—à–∞—è –ø–æ—Ç–µ—Ä—è —Ç–æ—á–Ω–æ—Å—Ç–∏
   - Pruning: —É–¥–∞–ª–µ–Ω–∏–µ –Ω–µ–≤–∞–∂–Ω—ã—Ö –≤–µ—Å–æ–≤ (5-20% —Å–æ–∫—Ä–∞—â–µ–Ω–∏–µ)
   - Knowledge Distillation: –æ–±—É—á–µ–Ω–∏–µ –º–∞–ª–µ–Ω—å–∫–æ–π –º–æ–¥–µ–ª–∏ –Ω–∞ –±–æ–ª—å—à–æ–π

5. –û–ø—Ç–∏–º–∞–ª—å–Ω–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è CPU (–ª–µ–≥–∫–∞—è, –Ω–æ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–∞—è):
   - num_encoder_layers=1
   - hidden_size=128
   - cnn_out_channels=256
   - –ö–≤–∞–Ω—Ç–∏–∑–∞—Ü–∏—è INT8
   –ò—Ç–æ–≥–æ: ~5-10 MB –≤–º–µ—Å—Ç–æ 40 MB
""")

print("=" * 80)
