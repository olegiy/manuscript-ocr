import os
import time
import csv
import re
import json
import torch
from collections import Counter, defaultdict
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    word_error_rate,
    compute_accuracy,
)
from manuscript.recognizers._trba.training.utils import load_checkpoint
from manuscript.recognizers._trba.data.dataset import OCRDatasetAttn
from manuscript.recognizers._trba.data.transforms import (
    load_charset,
    decode_tokens,
    get_val_transform,
)
from manuscript.recognizers._trba.model.model import TRBAModel
from torch.utils.data import DataLoader
import Levenshtein
from tqdm import tqdm


def normalize_text_letters_only(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
    –£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã.
    """
    letters_only = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ë–Å\u0080-\uFFFF]', '', text)
    return letters_only.lower()


# === –ü—É—Ç–∏ ===
datasets = [
    {
        "image_dir": r"C:\shared\orig_cyrillic\test",
        "gt_path": r"C:\shared\orig_cyrillic\test.csv",
    },
]

# PyTorch –º–æ–¥–µ–ª—å
weights_path = r"C:\Users\USER\Desktop\trba_exp_lite\best_acc_weights.pth"
config_path = r"C:\Users\USER\Desktop\trba_exp_lite\config.json"
charset_path = r"C:\Users\USER\Desktop\trba_exp_lite\charset.txt"

batch_size = 64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print(f"üñ•Ô∏è  Device: {device}")

# === –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ ===
print("\nüìã –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
with open(config_path, 'r', encoding='utf-8') as f:
    config = json.load(f)

max_len = config.get('max_len')
img_h = config.get('img_h')
img_w = config.get('img_w')
hidden_size = config.get('hidden_size')
num_encoder_layers = config.get('num_encoder_layers')
cnn_in_channels = config.get('cnn_in_channels')
cnn_out_channels = config.get('cnn_out_channels')
cnn_backbone = config.get('cnn_backbone')

print(f"   max_len: {max_len}")
print(f"   img_size: {img_h}x{img_w}")
print(f"   backbone: {cnn_backbone}")

# === –ó–∞–≥—Ä—É–∂–∞–µ–º charset ===
print("\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ charset...")
itos, stoi = load_charset(charset_path)
num_classes = len(itos)
PAD = stoi["<PAD>"]
SOS = stoi["<SOS>"]
EOS = stoi["<EOS>"]
BLANK = stoi.get("<BLANK>", None)

print(f"   –†–∞–∑–º–µ—Ä –∞–ª—Ñ–∞–≤–∏—Ç–∞: {num_classes} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: PAD={PAD}, SOS={SOS}, EOS={EOS}, BLANK={BLANK}")

# === –°–æ–∑–¥–∞—ë–º PyTorch –º–æ–¥–µ–ª—å ===
print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ PyTorch –º–æ–¥–µ–ª–∏...")
model = TRBAModel(
    num_classes=num_classes,
    hidden_size=hidden_size,
    num_encoder_layers=num_encoder_layers,
    img_h=img_h,
    img_w=img_w,
    cnn_in_channels=cnn_in_channels,
    cnn_out_channels=cnn_out_channels,
    cnn_backbone=cnn_backbone,
    use_ctc_head=False,
    use_attention_head=True,
)
model.to(device)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ç—É –∂–µ —Ñ—É–Ω–∫—Ü–∏—é —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ {os.path.basename(weights_path)}...")
checkpoint_data = load_checkpoint(
    path=weights_path,
    model=model,
    map_location=device,
    strict=False
)

model.eval()
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞")

# === –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å–ø–æ–ª—å–∑—É—è OCRDatasetAttn ===
print("\nüìÇ –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")

val_transform = get_val_transform(img_h, img_w)

image_dir = datasets[0]["image_dir"]
gt_path = datasets[0]["gt_path"]

print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç: {os.path.basename(image_dir)}")
print(f"   CSV: {os.path.basename(gt_path)}")

ocr_ds = OCRDatasetAttn(
    csv_path=gt_path,
    images_dir=image_dir,
    stoi=stoi,
    img_height=img_h,
    img_max_width=img_w,
    transform=val_transform,
    has_header=None,
    encoding="utf-8",
    delimiter=None,
    strict_charset=False,
    validate_image=False,
    max_len=max_len,
    strict_max_len=True,
    num_workers=0
)

print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ocr_ds)} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")

if hasattr(ocr_ds, '_reasons'):
    total_filtered = sum(ocr_ds._reasons.values())
    if total_filtered > 0:
        print(f"   ‚ö†Ô∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {total_filtered} –ø—Ä–∏–º–µ—Ä–æ–≤:")
        for reason, count in ocr_ds._reasons.items():
            if count > 0:
                print(f"      - {reason}: {count}")

# –°–æ–∑–¥–∞—ë–º collate_fn –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
collate_val = OCRDatasetAttn.make_collate_attn(
    stoi, max_len=max_len, drop_blank=True
)

dataloader = DataLoader(
    ocr_ds, 
    batch_size=batch_size, 
    shuffle=False, 
    num_workers=0,
    collate_fn=collate_val,
    pin_memory=False
)

# === –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ ===
print(f"\nüöÄ –ù–∞—á–∞–ª–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è (PyTorch)...")
start_time = time.perf_counter()

refs, hyps = [], []
error_details = []

# –°—á–µ—Ç—á–∏–∫ –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è –∏–Ω–¥–µ–∫—Å–∞ –ø—Ä–∏–º–µ—Ä–∞
sample_idx = 0

with torch.no_grad():
    for batch_imgs, text_in, target_y, lengths in tqdm(dataloader, desc="–†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ"):
        batch_imgs = batch_imgs.to(device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (–∫–∞–∫ –ø—Ä–∏ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –≤ –æ–±—É—á–µ–Ω–∏–∏)
        result = model(
            batch_imgs,
            is_train=False,
            batch_max_length=max_len,
            mode="attention"
        )
        
        pred_ids = result["attention_preds"].cpu()
        tgt_ids = target_y.cpu()
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (–∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)
        for pred_row, tgt_row in zip(pred_ids, tgt_ids):
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ—Ñ–µ—Ä–µ–Ω—Å
            ref_text = decode_tokens(tgt_row, itos, pad_id=PAD, eos_id=EOS, blank_id=BLANK)
            # –î–µ–∫–æ–¥–∏—Ä—É–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
            hyp_text = decode_tokens(pred_row, itos, pad_id=PAD, eos_id=EOS, blank_id=BLANK)
            
            refs.append(ref_text)
            hyps.append(hyp_text)
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
            img_path, _ = ocr_ds.samples[sample_idx]
            sample_idx += 1
            
            # –í—ã—á–∏—Å–ª—è–µ–º –æ—à–∏–±–∫–∏
            if ref_text != hyp_text:
                cer = character_error_rate(ref_text, hyp_text)
                wer = word_error_rate(ref_text, hyp_text)
                
                error_details.append({
                    'fname': os.path.basename(img_path),
                    'ref': ref_text,
                    'hyp': hyp_text,
                    'cer': cer,
                    'wer': wer,
                    'confidence': 0.0,  # –ù–µ—Ç confidence –≤ PyTorch —Ä–µ–∂–∏–º–µ
                    'dataset_id': 1  # –£ –Ω–∞—Å –æ–¥–∏–Ω –¥–∞—Ç–∞—Å–µ—Ç
                })

end_time = time.perf_counter()
total_time = end_time - start_time
avg_time = total_time / len(refs)
fps = 1.0 / avg_time if avg_time > 0 else float("inf")

print(f"\n‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ –∑–∞–≤–µ—Ä—à–µ–Ω–æ")
print(f"   –í—Ä–µ–º—è: {total_time:.2f} —Å–µ–∫")
print(f"   –°–∫–æ—Ä–æ—Å—Ç—å: {fps:.1f} FPS ({avg_time*1000:.1f} –º—Å/–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ)")

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
acc = compute_accuracy(refs, hyps)
acc_case_insensitive = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower()) / max(len(refs), 1)
acc_letters_only = sum(
    1 for r, h in zip(refs, hyps) 
    if normalize_text_letters_only(r) == normalize_text_letters_only(h)
) / max(len(refs), 1)

case_only_errors = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower() and r != h)

total_cer = sum(character_error_rate(r, h) for r, h in zip(refs, hyps))
total_wer = sum(word_error_rate(r, h) for r, h in zip(refs, hyps))
avg_cer = total_cer / max(len(refs), 1)
avg_wer = total_wer / max(len(refs), 1)

print("\n" + "="*120)
print("üìä –ú–ï–¢–†–ò–ö–ò (PyTorch)")
print("="*120)

# –í—ã—á–∏—Å–ª—è–µ–º CER case-insensitive –∏ letters-only –¥–ª—è overall
total_cer_ci = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps))
avg_cer_ci = total_cer_ci / max(len(refs), 1)

total_cer_letters = 0.0
for r, h in zip(refs, hyps):
    r_letters = normalize_text_letters_only(r)
    h_letters = normalize_text_letters_only(h)
    if r_letters:
        total_cer_letters += character_error_rate(r_letters, h_letters)
avg_cer_letters = total_cer_letters / max(len(refs), 1)

print(f"\n{'–ú–µ—Ç—Ä–∏–∫–∞':<30} {'–ó–Ω–∞—á–µ–Ω–∏–µ':>15}")
print("-" * 50)
print(f"{'–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤':<30} {len(refs):>15}")
print(f"{'Accuracy (case-sensitive)':<30} {acc*100:>14.2f}%")
print(f"{'Accuracy (case-insensitive)':<30} {acc_case_insensitive*100:>14.2f}%")
print(f"{'Accuracy (letters only)':<30} {acc_letters_only*100:>14.2f}%")
print(f"{'CER':<30} {avg_cer:>15.4f}")
print(f"{'CER (case-insensitive)':<30} {avg_cer_ci:>15.4f}")
print(f"{'CER (letters only)':<30} {avg_cer_letters:>15.4f}")
print(f"{'WER':<30} {avg_wer:>15.4f}")
print(f"{'Case-only errors':<30} {case_only_errors:>15}")
print("-" * 50)

print("\n–õ–µ–≥–µ–Ω–¥–∞:")
print("  Acc      - Accuracy (case-sensitive)")
print("  Acc-CI   - Accuracy (case-insensitive)")
print("  Acc-L    - Accuracy (letters only, case-insensitive)")
print("  CER      - Character Error Rate")
print("  CER-CI   - CER (case-insensitive)")
print("  CER-L    - CER (letters only)")
print("  WER      - Word Error Rate")

print("\n=== –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ===")
print(f"Case-only errors: {case_only_errors} ({case_only_errors/max(len(refs), 1)*100:.2f}%)")

# === –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã ===
if error_details:
    print(f"\nüî¥ –•–£–î–®–ò–ï –ü–†–ò–ú–ï–†–´ (—Ç–æ–ø-20 –ø–æ CER):")
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:20]
    for i, ex in enumerate(worst_examples, 1):
        print(f"   {i}. [{ex['fname']}]")
        print(f"      GT:   '{ex['ref']}'")
        print(f"      Pred: '{ex['hyp']}'")
        print(f"      CER: {ex['cer']:.3f}")
    
    print(f"\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(refs)}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(refs) - len(error_details)}")
    print(f"   –° –æ—à–∏–±–∫–∞–º–∏: {len(error_details)} ({len(error_details)/len(refs)*100:.1f}%)")
else:
    print("\n‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫! –í—Å–µ —Å–ª–æ–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã –∏–¥–µ–∞–ª—å–Ω–æ!")

print("\n" + "="*80)
print("‚úÖ PyTorch –≤–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
print("="*80)
