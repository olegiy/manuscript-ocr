"""
–ò–Ω—Ñ–µ—Ä–µ–Ω—Å TRBA –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º PTH –≤–µ—Å–æ–≤ –∏ –¥–µ—Ç–∞–ª—å–Ω—ã–º–∏ –º–µ—Ç—Ä–∏–∫–∞–º–∏.
–ê–Ω–∞–ª–æ–≥–∏—á–Ω–æ src/trba_metrics.py, –Ω–æ —Å –ø—Ä—è–º–æ–π –∑–∞–≥—Ä—É–∑–∫–æ–π PyTorch –º–æ–¥–µ–ª–∏.
"""

import os
import time
import csv
import json
from collections import Counter
from pathlib import Path
import torch
import cv2
import numpy as np
from PIL import Image
import Levenshtein
from tqdm import tqdm

from manuscript.recognizers._trba.model.model import TRBAModel
from manuscript.recognizers._trba.data.transforms import load_charset, get_val_transform
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    word_error_rate,
    compute_accuracy,
)

# ============================================
# –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø
# ============================================

# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–º
WEIGHTS_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\best_acc_weights.pth"
CONFIG_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\config.json"
CHARSET_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\charset.txt"

# –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
datasets = [
    {
        "image_dir": r"C:\shared\orig_cyrillic\test",
        "gt_path": r"C:\shared\orig_cyrillic\test.csv",
    },
    {
        "image_dir": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val",
        "gt_path": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val_converted.csv",
    },
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\labels.csv",
    },
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\labels.csv",
    },
]

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
BATCH_SIZE = 64
MAX_IMAGES = 10000000000000000  # –û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –Ω–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –æ—Ç—á–µ—Ç–æ–≤
OUTPUT_DIR = Path(WEIGHTS_PATH).parent

print("=" * 80)
print("üöÄ TRBA INFERENCE WITH PTH WEIGHTS + METRICS")
print("=" * 80)
print(f"Weights: {WEIGHTS_PATH}")
print(f"Config:  {CONFIG_PATH}")
print(f"Charset: {CHARSET_PATH}")
print(f"Device:  {DEVICE}")
print(f"Batch size: {BATCH_SIZE}")
print("=" * 80)
print()

# ============================================
# –ó–ê–ì–†–£–ó–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò
# ============================================

print("üìÑ Loading configuration...")
with open(CONFIG_PATH, "r", encoding="utf-8") as f:
    config = json.load(f)

img_h = config.get("img_h", 64)
img_w = config.get("img_w", 256)
max_len = config.get("max_len", 40)
hidden_size = config.get("hidden_size", 256)
num_encoder_layers = config.get("num_encoder_layers", 2)
cnn_in_channels = config.get("cnn_in_channels", 3)
cnn_out_channels = config.get("cnn_out_channels", 512)
cnn_backbone = config.get("cnn_backbone", "seresnet31")

print(f"   Image size: {img_h}√ó{img_w}")
print(f"   Max length: {max_len}")
print(f"   Hidden size: {hidden_size}")
print(f"   Encoder layers: {num_encoder_layers}")
print(f"   CNN backbone: {cnn_backbone}")
print(f"   CNN out channels: {cnn_out_channels}")
print()

# ============================================
# –ó–ê–ì–†–£–ó–ö–ê CHARSET
# ============================================

print("üìö Loading charset...")
itos, stoi = load_charset(CHARSET_PATH)
num_classes = len(itos)
print(f"   Total classes: {num_classes}")
print(f"   Special tokens: PAD={stoi['<PAD>']}, SOS={stoi['<SOS>']}, EOS={stoi['<EOS>']}")
print()

# ============================================
# –°–û–ó–î–ê–ù–ò–ï –ò –ó–ê–ì–†–£–ó–ö–ê –ú–û–î–ï–õ–ò
# ============================================

print("üèóÔ∏è  Building model...")
model = TRBAModel(
    num_classes=num_classes,
    hidden_size=hidden_size,
    num_encoder_layers=num_encoder_layers,
    img_h=img_h,
    img_w=img_w,
    cnn_in_channels=cnn_in_channels,
    cnn_out_channels=cnn_out_channels,
    cnn_backbone=cnn_backbone,
    sos_id=stoi["<SOS>"],
    eos_id=stoi["<EOS>"],
    pad_id=stoi["<PAD>"],
    blank_id=stoi.get("<BLANK>", None),
    use_ctc_head=False,  # –¢–æ–ª—å–∫–æ attention –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
    use_attention_head=True,
)

print(f"   Loading weights from {WEIGHTS_PATH}...")
state_dict = torch.load(WEIGHTS_PATH, map_location=DEVICE)
model.load_state_dict(state_dict, strict=False)
model.to(DEVICE)
model.eval()

# –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
total_params = sum(p.numel() for p in model.parameters())
print(f"   Total parameters: {total_params:,} ({total_params*4/(1024*1024):.2f} MB)")
print()

# ============================================
# –ó–ê–ì–†–£–ó–ö–ê GROUND TRUTH
# ============================================

print("üìÇ Loading ground truth data...")
gt_data = {}
total_gt_lines = 0

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    gt_path = dataset["gt_path"]
    
    print(f"   Dataset {idx}: {os.path.basename(image_dir)}")
    
    dataset_gt = {}
    with open(gt_path, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2:
                fname = row[0].strip()
                text = ",".join(row[1:]).strip()
                dataset_gt[fname] = text
    
    print(f"      Loaded {len(dataset_gt)} entries from {os.path.basename(gt_path)}")
    total_gt_lines += len(dataset_gt)
    
    for fname, text in dataset_gt.items():
        if fname in gt_data:
            print(f"      ‚ö†Ô∏è  Duplicate file: {fname} (using last version)")
        gt_data[fname] = text

print(f"\n   Total GT entries: {total_gt_lines}")
print(f"   Unique files: {len(gt_data)}")
print()

# ============================================
# –°–ö–ê–ù–ò–†–û–í–ê–ù–ò–ï –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–ô
# ============================================

print("üìÅ Scanning images...")
valid_ext = {".png", ".jpg", ".jpeg", ".tif", ".tiff", ".bmp"}
images = []

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    
    dataset_images = [
        os.path.join(image_dir, f)
        for f in os.listdir(image_dir)
        if os.path.splitext(f)[1].lower() in valid_ext
    ]
    
    if not dataset_images:
        print(f"   ‚ö†Ô∏è  Dataset {idx}: No images found in {image_dir}!")
    else:
        print(f"   Dataset {idx}: Found {len(dataset_images)} images")
        images.extend(dataset_images)

if len(images) > MAX_IMAGES:
    print(f"   ‚ö†Ô∏è  Taking only first {MAX_IMAGES} images from {len(images)}")
    images = images[:MAX_IMAGES]

if not images:
    raise RuntimeError(f"‚ùå No images found in any dataset!")

print(f"\n   TOTAL: {len(images)} images for recognition")
print()

# ============================================
# –ü–û–î–ì–û–¢–û–í–ö–ê –¢–†–ê–ù–°–§–û–†–ú–ê–¶–ò–ô
# ============================================

transform = get_val_transform(img_h=img_h, img_w=img_w)

def imread_unicode(path):
    """–ß–∏—Ç–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å Unicode –ø—É—Ç—ë–º (–ø–æ–¥–¥–µ—Ä–∂–∫–∞ –∫–∏—Ä–∏–ª–ª–∏—Ü—ã)"""
    with open(path, 'rb') as f:
        arr = np.frombuffer(f.read(), dtype=np.uint8)
        img = cv2.imdecode(arr, cv2.IMREAD_COLOR)
    return img

def preprocess_image(image_path):
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    img = imread_unicode(image_path)
    if img is None:
        raise ValueError(f"Failed to load image: {image_path}")
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
    transformed = transform(image=img)
    return transformed["image"]  # [3, H, W]

# ============================================
# INFERENCE
# ============================================

print("üîÆ Running inference...")
print(f"   Processing {len(images)} images in batches of {BATCH_SIZE}...")
print()

results = []
start_time = time.perf_counter()

with torch.no_grad():
    for i in tqdm(range(0, len(images), BATCH_SIZE), desc="Processing"):
        batch_images = images[i : i + BATCH_SIZE]
        
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –±–∞—Ç—á–∞
        batch_tensors = []
        for img_path in batch_images:
            try:
                tensor = preprocess_image(img_path)
                batch_tensors.append(tensor)
            except Exception as e:
                print(f"\n‚ö†Ô∏è  Error loading {img_path}: {e}")
                # –î–æ–±–∞–≤–ª—è–µ–º –ø—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                results.append({"text": "", "confidence": 0.0})
                continue
        
        if not batch_tensors:
            continue
        
        # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á [B, 3, H, W]
        batch = torch.stack(batch_tensors).to(DEVICE)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å
        output = model(batch, is_train=False, mode="attention", batch_max_length=max_len)
        preds = output["attention_preds"]  # [B, T]
        logits = output["attention_logits"]  # [B, T, num_classes]
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        probs = torch.softmax(logits, dim=-1).cpu().numpy()
        preds = preds.cpu().numpy()
        
        for j in range(len(batch_tensors)):
            pred_row = preds[j]  # [max_length]
            
            # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞
            decoded_chars = []
            for token_id in pred_row:
                if token_id == stoi["<EOS>"]:
                    break
                if token_id not in [stoi["<PAD>"], stoi["<SOS>"]]:
                    if token_id < len(itos):
                        decoded_chars.append(itos[token_id])
            
            text = "".join(decoded_chars)
            
            # –†–∞—Å—á–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
            seq_probs = []
            for t, token_id in enumerate(pred_row):
                if token_id == stoi["<EOS>"]:
                    break
                if token_id not in [stoi["<PAD>"], stoi["<SOS>"]]:
                    seq_probs.append(probs[j, t, token_id])
            
            confidence = float(np.mean(seq_probs)) if seq_probs else 0.0
            
            results.append({"text": text, "confidence": confidence})

end_time = time.perf_counter()
total_time = end_time - start_time
avg_time = total_time / len(images)
fps = 1.0 / avg_time if avg_time > 0 else float("inf")

print(f"\n‚úÖ Inference completed!")
print(f"   Total time: {total_time:.3f}s")
print(f"   Average per image: {avg_time:.3f}s ({fps:.1f} FPS)")
print()

# ============================================
# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò –¢–û–õ–¨–ö–û –ë–£–ö–í –ò –¶–ò–§–†
# ============================================

def filter_chars_only(text):
    """
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã (–≤–∫–ª—é—á–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –¥–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ) –∏ —Ü–∏—Ñ—Ä—ã.
    –£–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã: –±—É–∫–≤—ã –ª–∞—Ç–∏–Ω–∏—Ü—ã, –∫–∏—Ä–∏–ª–ª–∏—Ü—ã (–≤–∫–ª—é—á–∞—è –¥–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ) –∏ —Ü–∏—Ñ—Ä—ã
    allowed_chars = set(
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
        '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'
        '—£—¢—ñ–Ü—≥—≤—µ—¥—´—™—≠—¨—Ø—Æ—±—∞—°—†—ï—ï—ß—¶—©—®'  # –î–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        '0123456789'
    )
    return ''.join(c for c in text if c in allowed_chars)

# ============================================
# –°–û–ü–û–°–¢–ê–í–õ–ï–ù–ò–ï –° GROUND TRUTH –ò –†–ê–°–ß–ï–¢ –ú–ï–¢–†–ò–ö
# ============================================

print("=" * 80)
print("üìä CALCULATING METRICS")
print("=" * 80)
print()

refs, hyps = [], []
total_cer, total_wer = 0.0, 0.0
cer_count, wer_count = 0, 0
error_details = []

# –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç–∏
dataset_mapping = {}  # –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é -> —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset_results = {}  # —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è -> –º–µ—Ç—Ä–∏–∫–∏

for idx, dataset in enumerate(datasets, 1):
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è: –µ—Å–ª–∏ –±–∞–∑–æ–≤–æ–µ –∏–º—è –Ω–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—É—Ç–∏
    base_name = os.path.basename(dataset["image_dir"])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏
    existing_names = [name for name in dataset_results.keys()]
    if base_name in existing_names:
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å –ø—É—Ç–∏ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        parent = os.path.basename(os.path.dirname(dataset["image_dir"]))
        dataset_name = f"{parent}_{base_name}"
    else:
        dataset_name = base_name
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    normalized_dir = os.path.normpath(dataset["image_dir"])
    dataset_mapping[normalized_dir] = dataset_name
    
    dataset_results[dataset_name] = {
        'refs': [],
        'hyps': [],
        'total_cer': 0.0,
        'total_wer': 0.0,
        'count': 0
    }

print("Results:")
print("-" * 80)
for path, result in zip(images, results):
    pred_text = result["text"]
    score = result["confidence"]
    fname = os.path.basename(path)
    ref_text = gt_data.get(fname)
    
    if ref_text is None:
        print(f"{fname:40s} ‚Üí {pred_text:20s} (no GT)")
        continue

    refs.append(ref_text)
    hyps.append(pred_text)

    cer = character_error_rate(ref_text, pred_text)
    wer = word_error_rate(ref_text, pred_text)

    total_cer += cer
    total_wer += wer
    cer_count += 1
    wer_count += 1
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    normalized_path = os.path.normpath(path)
    path_dir = os.path.dirname(normalized_path)
    
    dataset_name = dataset_mapping.get(path_dir)
    if dataset_name:
        dataset_results[dataset_name]['refs'].append(ref_text)
        dataset_results[dataset_name]['hyps'].append(pred_text)
        dataset_results[dataset_name]['total_cer'] += cer
        dataset_results[dataset_name]['total_wer'] += wer
        dataset_results[dataset_name]['count'] += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –æ—à–∏–±–æ–∫
    if ref_text != pred_text:
        error_details.append({
            'fname': fname,
            'ref': ref_text,
            'hyp': pred_text,
            'cer': cer,
            'wer': wer,
            'confidence': score
        })

    print(f"{fname:40s} ‚Üí {pred_text:20s} | GT: {ref_text:20s} | CER={cer:.3f} | WER={wer:.3f}")

print("-" * 80)
print()

# ============================================
# –û–°–ù–û–í–ù–´–ï –ú–ï–¢–†–ò–ö–ò
# ============================================

acc = compute_accuracy(refs, hyps)
acc_case_insensitive = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower()) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ –±—É–∫–≤–∞–º –∏ —Ü–∏—Ñ—Ä–∞–º (chars only)
refs_chars_only = [filter_chars_only(r) for r in refs]
hyps_chars_only = [filter_chars_only(h) for h in hyps]
acc_chars_only = sum(1 for r, h in zip(refs_chars_only, hyps_chars_only) if r.lower() == h.lower()) / max(len(refs), 1)

case_only_errors = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower() and r != h)
avg_cer = total_cer / max(cer_count, 1)
avg_wer = total_wer / max(wer_count, 1)

# CER/WER –¥–ª—è case-insensitive
total_cer_ci = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps)) / max(len(refs), 1)
total_wer_ci = sum(word_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps)) / max(len(refs), 1)

# CER/WER –¥–ª—è chars only (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—ã —Å –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏)
total_cer_chars_only = 0.0
total_wer_chars_only = 0.0
chars_only_count = 0

for r, h in zip(refs_chars_only, hyps_chars_only):
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—ã –≥–¥–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –ø—É—Å—Ç–∞—è
    if not r and not h:
        # –û–±–µ –ø—É—Å—Ç—ã–µ - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (CER=0, WER=0)
        chars_only_count += 1
        continue
    elif not r or not h:
        # –û–¥–Ω–∞ –ø—É—Å—Ç–∞—è, –¥—Ä—É–≥–∞—è –Ω–µ—Ç - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –ø–æ–ª–Ω—É—é –æ—à–∏–±–∫—É
        total_cer_chars_only += 1.0
        total_wer_chars_only += 1.0
        chars_only_count += 1
        continue
    
    # –û–±–µ –Ω–µ–ø—É—Å—Ç—ã–µ - —Å—á–∏—Ç–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    total_cer_chars_only += character_error_rate(r, h)
    total_wer_chars_only += word_error_rate(r, h)
    chars_only_count += 1

total_cer_chars_only = total_cer_chars_only / max(chars_only_count, 1)
total_wer_chars_only = total_wer_chars_only / max(chars_only_count, 1)

print("=" * 80)
print("üìà SUMMARY METRICS")
print("=" * 80)
print(f"Accuracy (case-sensitive):     {acc*100:.2f}%")
print(f"Accuracy (case-insensitive):   {acc_case_insensitive*100:.2f}%")
print(f"Accuracy (chars only):         {acc_chars_only*100:.2f}%")
print(f"Case-only errors:              {case_only_errors} ({case_only_errors/max(len(refs), 1)*100:.2f}%)")
print(f"Avg CER:  {avg_cer:.4f}")
print(f"Avg WER:  {avg_wer:.4f}")
print(f"Processed {len(images)} images in {total_time:.3f} sec")
print(f"Average per image: {avg_time:.3f} sec ({fps:.1f} FPS)")
print("=" * 80)
print()

# ============================================
# –¢–ê–ë–õ–ò–¶–ê –ú–ï–¢–†–ò–ö –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú
# ============================================

print("=" * 100)
print("üìä –ú–ï–¢–†–ò–ö–ò –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú")
print("=" * 100)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
metrics_table = []

for dataset_name, data in dataset_results.items():
    if data['count'] == 0:
        continue
    
    # Accuracy (case-sensitive)
    acc_ds = compute_accuracy(data['refs'], data['hyps'])
    
    # Accuracy (case-insensitive)
    acc_ci_ds = sum(1 for r, h in zip(data['refs'], data['hyps']) if r.lower() == h.lower()) / max(data['count'], 1)
    
    # Accuracy (chars only)
    refs_co_ds = [filter_chars_only(r) for r in data['refs']]
    hyps_co_ds = [filter_chars_only(h) for h in data['hyps']]
    acc_co_ds = sum(1 for r, h in zip(refs_co_ds, hyps_co_ds) if r.lower() == h.lower()) / max(data['count'], 1)
    
    # CER/WER
    avg_cer_ds = data['total_cer'] / data['count']
    avg_wer_ds = data['total_wer'] / data['count']
    
    # CER/WER (case-insensitive)
    cer_ci_ds = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(data['refs'], data['hyps'])) / data['count']
    wer_ci_ds = sum(word_error_rate(r.lower(), h.lower()) for r, h in zip(data['refs'], data['hyps'])) / data['count']
    
    # CER/WER (chars only) - —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    total_cer_co_ds = 0.0
    total_wer_co_ds = 0.0
    co_count_ds = 0
    
    for r, h in zip(refs_co_ds, hyps_co_ds):
        if not r and not h:
            co_count_ds += 1
            continue
        elif not r or not h:
            total_cer_co_ds += 1.0
            total_wer_co_ds += 1.0
            co_count_ds += 1
            continue
        total_cer_co_ds += character_error_rate(r, h)
        total_wer_co_ds += word_error_rate(r, h)
        co_count_ds += 1
    
    cer_co_ds = total_cer_co_ds / max(co_count_ds, 1)
    wer_co_ds = total_wer_co_ds / max(co_count_ds, 1)
    
    metrics_table.append({
        'Dataset': dataset_name,
        'Count': data['count'],
        'Acc (CS)': acc_ds,
        'Acc (CI)': acc_ci_ds,
        'Acc (CO)': acc_co_ds,
        'CER (CS)': avg_cer_ds,
        'CER (CI)': cer_ci_ds,
        'CER (CO)': cer_co_ds,
        'WER (CS)': avg_wer_ds,
        'WER (CI)': wer_ci_ds,
        'WER (CO)': wer_co_ds,
    })

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç—Ä–æ–∫—É (TOTAL)
metrics_table.append({
    'Dataset': 'TOTAL',
    'Count': len(refs),
    'Acc (CS)': acc,
    'Acc (CI)': acc_case_insensitive,
    'Acc (CO)': acc_chars_only,
    'CER (CS)': avg_cer,
    'CER (CI)': total_cer_ci,
    'CER (CO)': total_cer_chars_only,
    'WER (CS)': avg_wer,
    'WER (CI)': total_wer_ci,
    'WER (CO)': total_wer_chars_only,
})

# –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã
print(f"\n{'Dataset':<30} {'Count':>6} {'Acc(CS)':>8} {'Acc(CI)':>8} {'Acc(CO)':>8} {'CER(CS)':>8} {'CER(CI)':>8} {'CER(CO)':>8} {'WER(CS)':>8} {'WER(CI)':>8} {'WER(CO)':>8}")
print("-" * 100)
for row in metrics_table:
    is_total = row['Dataset'] == 'TOTAL'
    sep = "=" if is_total else "-"
    if is_total:
        print(sep * 100)
    print(f"{row['Dataset']:<30} {row['Count']:>6} {row['Acc (CS)']*100:>7.2f}% {row['Acc (CI)']*100:>7.2f}% {row['Acc (CO)']*100:>7.2f}% "
          f"{row['CER (CS)']:>8.4f} {row['CER (CI)']:>8.4f} {row['CER (CO)']:>8.4f} "
          f"{row['WER (CS)']:>8.4f} {row['WER (CI)']:>8.4f} {row['WER (CO)']:>8.4f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ CSV
csv_output_path = OUTPUT_DIR / "metrics_by_dataset.csv"
with open(csv_output_path, "w", newline="", encoding="utf-8") as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=[
        'Dataset', 'Count', 
        'Acc (CS)', 'Acc (CI)', 'Acc (CO)',
        'CER (CS)', 'CER (CI)', 'CER (CO)',
        'WER (CS)', 'WER (CI)', 'WER (CO)'
    ])
    writer.writeheader()
    for row in metrics_table:
        writer.writerow(row)

print(f"\nüíæ –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_output_path}")
print("=" * 100)
print()

# ============================================
# –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
# ============================================

def analyze_character_errors(refs, hyps):
    """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    substitutions = Counter()
    insertions = Counter()
    deletions = Counter()
    error_positions = {'start': 0, 'middle': 0, 'end': 0}
    
    for ref, hyp in zip(refs, hyps):
        if ref == hyp:
            continue
            
        ops = Levenshtein.editops(ref, hyp)
        
        for op_type, ref_pos, hyp_pos in ops:
            word_len = len(ref)
            if ref_pos < word_len * 0.2:
                error_positions['start'] += 1
            elif ref_pos > word_len * 0.8:
                error_positions['end'] += 1
            else:
                error_positions['middle'] += 1
            
            if op_type == 'replace':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                substitutions[(ref_char, hyp_char)] += 1
            elif op_type == 'insert':
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                insertions[hyp_char] += 1
            elif op_type == 'delete':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                deletions[ref_char] += 1
    
    return substitutions, insertions, deletions, error_positions


def analyze_word_lengths(error_details):
    """–ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏"""
    return [len(detail['ref']) for detail in error_details]


def analyze_error_types(error_details):
    """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫"""
    total_errors = len(error_details)
    length_mismatch = 0
    case_errors = 0
    similar_chars = 0
    completely_wrong = 0
    
    for detail in error_details:
        ref = detail['ref']
        hyp = detail['hyp']
        
        if len(ref) != len(hyp):
            length_mismatch += 1
        elif ref.lower() == hyp.lower():
            case_errors += 1
        else:
            distance = Levenshtein.distance(ref, hyp)
            if distance <= 2:
                similar_chars += 1
            else:
                completely_wrong += 1
    
    return {
        'total': total_errors,
        'length_mismatch': length_mismatch,
        'case_errors': case_errors,
        'similar_chars': similar_chars,
        'completely_wrong': completely_wrong
    }


if error_details:
    print("=" * 80)
    print("üìä DETAILED ERROR ANALYSIS")
    print("=" * 80)
    
    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    print(f"\n1Ô∏è‚É£ GENERAL STATISTICS:")
    print(f"   Total examples: {len(refs)}")
    print(f"   Correct: {len(refs) - len(error_details)}")
    print(f"   With errors: {len(error_details)} ({len(error_details)/len(refs)*100:.1f}%)")
    
    print(f"\n   üìè Metrics (case-sensitive):")
    print(f"      Accuracy: {acc*100:.2f}%")
    print(f"      CER: {avg_cer:.4f}")
    print(f"      WER: {avg_wer:.4f}")
    
    print(f"\n   üìè Metrics (case-insensitive):")
    print(f"      Accuracy: {acc_case_insensitive*100:.2f}%")
    print(f"      CER: {total_cer_ci:.4f}")
    print(f"      WER: {total_wer_ci:.4f}")
    if avg_cer > 0:
        print(f"      CER improvement: {(avg_cer - total_cer_ci)/avg_cer*100:.1f}%")
    if avg_wer > 0:
        print(f"      WER improvement: {(avg_wer - total_wer_ci)/avg_wer*100:.1f}%")
    
    print(f"\n   üìè Metrics (chars only - no special chars/spaces):")
    print(f"      Accuracy: {acc_chars_only*100:.2f}%")
    print(f"      CER: {total_cer_chars_only:.4f}")
    print(f"      WER: {total_wer_chars_only:.4f}")
    if avg_cer > 0:
        print(f"      CER improvement: {(avg_cer - total_cer_chars_only)/avg_cer*100:.1f}%")
    if avg_wer > 0:
        print(f"      WER improvement: {(avg_wer - total_wer_chars_only)/avg_wer*100:.1f}%")
    
    # 2. –¢–∏–ø—ã –æ—à–∏–±–æ–∫
    print(f"\n2Ô∏è‚É£ ERROR TYPES:")
    error_types = analyze_error_types(error_details)
    print(f"   Different length: {error_types['length_mismatch']} ({error_types['length_mismatch']/error_types['total']*100:.1f}%)")
    print(f"   Case only: {error_types['case_errors']} ({error_types['case_errors']/error_types['total']*100:.1f}%)")
    print(f"   Similar (1-2 chars): {error_types['similar_chars']} ({error_types['similar_chars']/error_types['total']*100:.1f}%)")
    print(f"   Completely wrong: {error_types['completely_wrong']} ({error_types['completely_wrong']/error_types['total']*100:.1f}%)")
    
    # 3. –î–ª–∏–Ω–∞ —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    print(f"\n3Ô∏è‚É£ ERROR WORD LENGTHS:")
    error_lengths = analyze_word_lengths(error_details)
    if error_lengths:
        avg_error_len = sum(error_lengths) / len(error_lengths)
        print(f"   Average length: {avg_error_len:.1f} characters")
        print(f"   Min: {min(error_lengths)}, Max: {max(error_lengths)}")
        
        length_dist = Counter(error_lengths)
        print(f"   Distribution (top-10):")
        for length in sorted(length_dist.keys())[:10]:
            print(f"      {length} characters: {length_dist[length]} words")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    print(f"\n4Ô∏è‚É£ CHARACTER-LEVEL ERROR ANALYSIS:")
    substitutions, insertions, deletions, error_positions = analyze_character_errors(refs, hyps)
    
    total_pos = sum(error_positions.values())
    if total_pos > 0:
        print(f"   Error position in word:")
        print(f"      Start (0-20%): {error_positions['start']} ({error_positions['start']/total_pos*100:.1f}%)")
        print(f"      Middle (20-80%): {error_positions['middle']} ({error_positions['middle']/total_pos*100:.1f}%)")
        print(f"      End (80-100%): {error_positions['end']} ({error_positions['end']/total_pos*100:.1f}%)")
    
    print(f"\n   üîÑ Top-20 character substitutions (correct ‚Üí wrong):")
    
    case_substitutions = []
    non_case_substitutions = []
    
    for (correct, wrong), count in substitutions.items():
        if correct.lower() == wrong.lower() and correct != wrong:
            case_substitutions.append(((correct, wrong), count))
        else:
            non_case_substitutions.append(((correct, wrong), count))
    
    case_substitutions.sort(key=lambda x: x[1], reverse=True)
    non_case_substitutions.sort(key=lambda x: x[1], reverse=True)
    
    if case_substitutions:
        print(f"\n      Case substitutions (top-10):")
        for (correct, wrong), count in case_substitutions[:10]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} times")
    
    if non_case_substitutions:
        print(f"\n      Other substitutions (top-20):")
        for (correct, wrong), count in non_case_substitutions[:20]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} times")
    
    if insertions:
        print(f"\n   ‚ûï Top-10 inserted characters:")
        for char, count in insertions.most_common(10):
            print(f"      '{char}': {count} times")
    
    if deletions:
        print(f"\n   ‚ûñ Top-10 deleted characters:")
        for char, count in deletions.most_common(10):
            print(f"      '{char}': {count} times")
    
    # 5. –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\n5Ô∏è‚É£ WORST EXAMPLES (top-10 by CER):")
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:10]
    for i, ex in enumerate(worst_examples, 1):
        print(f"   {i}. [{ex['fname']}]")
        print(f"      GT:   '{ex['ref']}'")
        print(f"      Pred: '{ex['hyp']}'")
        print(f"      CER: {ex['cer']:.3f}, Conf: {ex['confidence']:.3f}")
    
    # 6. –°–≤—è–∑—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–æ–∫
    print(f"\n6Ô∏è‚É£ CONFIDENCE VS ERRORS:")
    low_conf_errors = [e for e in error_details if e['confidence'] < 0.8]
    high_conf_errors = [e for e in error_details if e['confidence'] >= 0.8]
    print(f"   Errors with low confidence (<0.8): {len(low_conf_errors)} ({len(low_conf_errors)/len(error_details)*100:.1f}%)")
    print(f"   Errors with high confidence (‚â•0.8): {len(high_conf_errors)} ({len(high_conf_errors)/len(error_details)*100:.1f}%)")
    
    if error_details:
        avg_conf_errors = sum(e['confidence'] for e in error_details) / len(error_details)
        print(f"   Average confidence on errors: {avg_conf_errors:.3f}")
    
    # 7. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ CSV
    print(f"\n7Ô∏è‚É£ SAVING ERROR DETAILS TO CSV...")
    sorted_errors = sorted(error_details, key=lambda x: x['confidence'], reverse=True)
    
    output_csv = OUTPUT_DIR / "ocr_errors_by_confidence.csv"
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["filename", "confidence", "CER", "WER", "GT", "Prediction"])
        for err in sorted_errors:
            writer.writerow([
                err['fname'],
                f"{err['confidence']:.4f}",
                f"{err['cer']:.4f}",
                f"{err['wer']:.4f}",
                err['ref'],
                err['hyp'],
            ])
    
    print(f"   üíæ Errors saved to: {output_csv}")
    
    print()
    print("=" * 80)
else:
    print("‚úÖ NO ERRORS! All words recognized perfectly!")
    print("=" * 80)

print("\n‚ú® DONE!")
