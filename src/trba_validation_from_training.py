"""
–ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏—Å–ø–æ–ª—å–∑—É—è –¢–£ –ñ–ï –ª–æ–≥–∏–∫—É —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
–ü–æ–∑–≤–æ–ª—è–µ—Ç –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ—Å—Ç–∏ —Ç–æ—á–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –æ–±—É—á–µ–Ω–∏—è.
"""
import os
import json
import torch
import logging
from pathlib import Path

from manuscript.recognizers._trba.model.model import TRBAModel
from manuscript.recognizers._trba.data.dataset import OCRDatasetAttn
from manuscript.recognizers._trba.data.transforms import (
    load_charset,
    decode_tokens,
    get_train_transform,
    get_val_transform,
)
from manuscript.recognizers._trba.training.utils import load_checkpoint
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    word_error_rate,
    compute_accuracy,
)
from torch.utils.data import DataLoader
from tqdm import tqdm

# === –ù–ê–°–¢–†–û–ô–ö–ò ===
# –ü—É—Ç—å –∫ —á–µ–∫–ø–æ–∏–Ω—Ç—É (–º–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å best_acc_ckpt.pth –∏–ª–∏ last_ckpt.pth)
checkpoint_path = r"C:\Users\USER\Desktop\trba_exp_lite\best_acc_ckpt.pth"

# –ü—É—Ç—å –∫ –¥–∞—Ç–∞—Å–µ—Ç—É –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
val_csv = r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\labels.csv"
val_images_dir = r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\img"

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã
batch_size = 64
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

print("="*80)
print("üîÑ –í–ê–õ–ò–î–ê–¶–ò–Ø –° –ò–°–ü–û–õ–¨–ó–û–í–ê–ù–ò–ï–ú –õ–û–ì–ò–ö–ò –û–ë–£–ß–ï–ù–ò–Ø")
print("="*80)
print(f"üñ•Ô∏è  Device: {device}")
print(f"üì¶ Checkpoint: {os.path.basename(checkpoint_path)}")
print(f"üìÇ Validation CSV: {os.path.basename(val_csv)}")
print(f"üìÅ Images dir: {os.path.basename(val_images_dir)}")

# === –ó–∞–≥—Ä—É–∂–∞–µ–º —á–µ–∫–ø–æ–∏–Ω—Ç ===
print("\nüì¶ –ó–∞–≥—Ä—É–∑–∫–∞ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
checkpoint = torch.load(checkpoint_path, map_location='cpu', weights_only=False)

# –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–æ–Ω—Ñ–∏–≥ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
if 'config' in checkpoint:
    config = checkpoint['config']
    print("‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞")
else:
    # –ü—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ config.json —Ä—è–¥–æ–º —Å —á–µ–∫–ø–æ–∏–Ω—Ç–æ–º
    config_path = Path(checkpoint_path).parent / "config.json"
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
        print(f"‚úÖ –ö–æ–Ω—Ñ–∏–≥ –∑–∞–≥—Ä—É–∂–µ–Ω –∏–∑ {config_path.name}")
    else:
        raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω config –≤ —á–µ–∫–ø–æ–∏–Ω—Ç–µ –∏ —Ñ–∞–π–ª {config_path}")

# –ò–∑–≤–ª–µ–∫–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
max_len = config.get('max_len')
img_h = config.get('img_h')
img_w = config.get('img_w')
hidden_size = config.get('hidden_size')
num_encoder_layers = config.get('num_encoder_layers')
cnn_in_channels = config.get('cnn_in_channels', 1)
cnn_out_channels = config.get('cnn_out_channels', 512)
cnn_backbone = config.get('cnn_backbone', 'seresnet31lite')

print(f"\nüìã –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏:")
print(f"   max_len: {max_len}")
print(f"   img_size: {img_h}x{img_w}")
print(f"   hidden_size: {hidden_size}")
print(f"   num_encoder_layers: {num_encoder_layers}")
print(f"   backbone: {cnn_backbone}")

# –ó–∞–≥—Ä—É–∂–∞–µ–º charset
charset_path = Path(checkpoint_path).parent / "charset.txt"
if not charset_path.exists():
    raise FileNotFoundError(f"–ù–µ –Ω–∞–π–¥–µ–Ω —Ñ–∞–π–ª charset: {charset_path}")

print(f"\nüìö –ó–∞–≥—Ä—É–∑–∫–∞ charset –∏–∑ {charset_path.name}...")
itos, stoi = load_charset(str(charset_path))
num_classes = len(itos)
PAD = stoi["<PAD>"]
SOS = stoi["<SOS>"]
EOS = stoi["<EOS>"]
BLANK = stoi.get("<BLANK>", None)

print(f"   –†–∞–∑–º–µ—Ä –∞–ª—Ñ–∞–≤–∏—Ç–∞: {num_classes} —Å–∏–º–≤–æ–ª–æ–≤")
print(f"   –°–ø–µ—Ü–∏–∞–ª—å–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: PAD={PAD}, SOS={SOS}, EOS={EOS}, BLANK={BLANK}")

# –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ
if 'epoch' in checkpoint:
    print(f"\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —á–µ–∫–ø–æ–∏–Ω—Ç–µ:")
    print(f"   –≠–ø–æ—Ö–∞: {checkpoint['epoch']}")
    if 'val_acc' in checkpoint:
        print(f"   Val Accuracy: {checkpoint['val_acc']*100:.2f}%")
    if 'val_cer' in checkpoint:
        print(f"   Val CER: {checkpoint['val_cer']:.4f}")
    if 'val_loss' in checkpoint:
        print(f"   Val Loss: {checkpoint['val_loss']:.4f}")

# === –°–æ–∑–¥–∞—ë–º –º–æ–¥–µ–ª—å ===
print("\nüîß –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏...")
model = TRBAModel(
    num_classes=num_classes,
    hidden_size=hidden_size,
    num_encoder_layers=num_encoder_layers,
    img_h=img_h,
    img_w=img_w,
    cnn_in_channels=cnn_in_channels,
    cnn_out_channels=cnn_out_channels,
    cnn_backbone=cnn_backbone,
    use_ctc_head=False,
    use_attention_head=True,
)
model.to(device)

# –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ –∏—Å–ø–æ–ª—å–∑—É—è —Ñ—É–Ω–∫—Ü–∏—é –∏–∑ –æ–±—É—á–µ–Ω–∏—è
print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞...")
load_checkpoint(
    path=checkpoint_path,
    model=model,
    map_location=device,
    strict=False
)

model.eval()
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏ –≥–æ—Ç–æ–≤–∞ –∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏")

# === –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ (–ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò) ===
print("\nüìÇ –°–æ–∑–¥–∞–Ω–∏–µ –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
val_transform = get_val_transform(img_h, img_w)

val_dataset = OCRDatasetAttn(
    csv_path=val_csv,
    images_dir=val_images_dir,
    stoi=stoi,
    img_height=img_h,
    img_max_width=img_w,
    transform=val_transform,
    has_header=None,
    encoding="utf-8",
    delimiter=None,
    strict_charset=False,
    validate_image=False,
    max_len=max_len,
    strict_max_len=True,
    num_workers=0
)

print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(val_dataset)} –ø—Ä–∏–º–µ—Ä–æ–≤")

# –°–æ–∑–¥–∞—ë–º collate_fn –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
collate_val = OCRDatasetAttn.make_collate_attn(
    stoi, max_len=max_len, drop_blank=True
)

val_loader = DataLoader(
    val_dataset,
    batch_size=batch_size,
    shuffle=False,
    num_workers=0,
    collate_fn=collate_val,
    pin_memory=False
)

print(f"   DataLoader —Å–æ–∑–¥–∞–Ω: {len(val_loader)} –±–∞—Ç—á–µ–π")

# === –í–ê–õ–ò–î–ê–¶–ò–Ø (–¢–û–ß–ù–û –ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò) ===
print("\nüöÄ –ó–∞–ø—É—Å–∫ –≤–∞–ª–∏–¥–∞—Ü–∏–∏...")
print("="*80)

model.eval()
refs = []
hyps = []
total_loss = 0.0

with torch.no_grad():
    for imgs, text_in, target_y, lengths in tqdm(val_loader, desc="–í–∞–ª–∏–¥–∞—Ü–∏—è"):
        imgs = imgs.to(device)
        
        # –ò–Ω—Ñ–µ—Ä–µ–Ω—Å (–ö–ê–ö –í –û–ë–£–ß–ï–ù–ò–ò)
        result = model(
            imgs,
            is_train=False,
            batch_max_length=max_len,
            mode="attention"
        )
        
        # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
        pred_ids = result["attention_preds"].cpu()
        tgt_ids = target_y.cpu()
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (–ö–ê–ö –í –û–ë–£–ß–ï–ù–ò–ò)
        for pred_row, tgt_row in zip(pred_ids, tgt_ids):
            ref_text = decode_tokens(
                tgt_row, itos, pad_id=PAD, eos_id=EOS, blank_id=BLANK
            )
            hyp_text = decode_tokens(
                pred_row, itos, pad_id=PAD, eos_id=EOS, blank_id=BLANK
            )
            refs.append(ref_text)
            hyps.append(hyp_text)

# === –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ (–ö–ê–ö –í –û–ë–£–ß–ï–ù–ò–ò) ===
print("\nüìä –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫...")

val_acc = compute_accuracy(refs, hyps)
val_cer = sum(
    character_error_rate(r, h) for r, h in zip(refs, hyps)
) / max(1, len(refs))
val_wer = sum(
    word_error_rate(r, h) for r, h in zip(refs, hyps)
) / max(1, len(refs))

# –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏
acc_case_insensitive = sum(
    1 for r, h in zip(refs, hyps) if r.lower() == h.lower()
) / max(len(refs), 1)

correct_count = sum(1 for r, h in zip(refs, hyps) if r == h)

print("\n" + "="*80)
print("üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ –í–ê–õ–ò–î–ê–¶–ò–ò (–∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏)")
print("="*80)
print(f"\n{'–ú–µ—Ç—Ä–∏–∫–∞':<40} {'–ó–Ω–∞—á–µ–Ω–∏–µ':>15}")
print("-" * 60)
print(f"{'–í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤':<40} {len(refs):>15}")
print(f"{'–ü—Ä–∞–≤–∏–ª—å–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω–æ':<40} {correct_count:>15}")
print(f"{'–° –æ—à–∏–±–∫–∞–º–∏':<40} {len(refs) - correct_count:>15}")
print("-" * 60)
print(f"{'Accuracy (case-sensitive)':<40} {val_acc*100:>14.2f}%")
print(f"{'Accuracy (case-insensitive)':<40} {acc_case_insensitive*100:>14.2f}%")
print(f"{'Character Error Rate (CER)':<40} {val_cer:>15.4f}")
print(f"{'Word Error Rate (WER)':<40} {val_wer:>15.4f}")
print("="*80)

# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏–∑ —á–µ–∫–ø–æ–∏–Ω—Ç–∞
if 'val_acc' in checkpoint:
    print("\nüìà –°–†–ê–í–ù–ï–ù–ò–ï –° –ß–ï–ö–ü–û–ò–ù–¢–û–ú:")
    print("-" * 60)
    ckpt_acc = checkpoint.get('val_acc', 0.0)
    ckpt_cer = checkpoint.get('val_cer', 0.0)
    ckpt_wer = checkpoint.get('val_wer', 0.0)
    
    print(f"{'–ú–µ—Ç—Ä–∏–∫–∞':<30} {'–ß–µ–∫–ø–æ–∏–Ω—Ç':>15} {'–¢–µ–∫—É—â–∞—è':>15} {'Œî':>10}")
    print("-" * 60)
    print(f"{'Accuracy':<30} {ckpt_acc*100:>14.2f}% {val_acc*100:>14.2f}% {(val_acc-ckpt_acc)*100:>9.2f}%")
    if ckpt_cer > 0:
        print(f"{'CER':<30} {ckpt_cer:>15.4f} {val_cer:>15.4f} {val_cer-ckpt_cer:>10.4f}")
    if ckpt_wer > 0:
        print(f"{'WER':<30} {ckpt_wer:>15.4f} {val_wer:>15.4f} {val_wer-ckpt_wer:>10.4f}")
    print("="*80)

# === –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã ===
print("\nüî¥ –•–£–î–®–ò–ï –ü–†–ò–ú–ï–†–´ (—Ç–æ–ø-20 –ø–æ CER):")
errors = []
for i, (r, h) in enumerate(zip(refs, hyps)):
    if r != h:
        cer = character_error_rate(r, h)
        img_path, _ = val_dataset.samples[i]
        errors.append({
            'fname': os.path.basename(img_path),
            'ref': r,
            'hyp': h,
            'cer': cer
        })

worst_errors = sorted(errors, key=lambda x: x['cer'], reverse=True)[:20]
for i, err in enumerate(worst_errors, 1):
    print(f"   {i}. [{err['fname']}]")
    print(f"      GT:   '{err['ref']}'")
    print(f"      Pred: '{err['hyp']}'")
    print(f"      CER: {err['cer']:.3f}")

print("\n" + "="*80)
print("‚úÖ –í–∞–ª–∏–¥–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞")
print("="*80)
