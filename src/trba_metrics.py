import os
import time
import csv
from collections import Counter, defaultdict
from manuscript.recognizers import TRBA
from manuscript.recognizers._trba.training.metrics import (
    compute_cer,
    compute_wer,
    compute_accuracy,
)
from tqdm import tqdm

# Levenshtein only for detailed error analysis
try:
    import Levenshtein
    HAS_LEVENSHTEIN = True
except ImportError:
    HAS_LEVENSHTEIN = False
    print("Warning: python-Levenshtein not installed. Detailed error analysis will be limited.")


# Wrapper functions for single-item compatibility
def character_error_rate(reference: str, hypothesis: str) -> float:
    """Single-item CER for compatibility."""
    return compute_cer([reference], [hypothesis])


def word_error_rate(reference: str, hypothesis: str) -> float:
    """Single-item WER for compatibility."""
    return compute_wer([reference], [hypothesis])


# === –ü—É—Ç–∏ ===
# –ü—É—Ç–∏ –∫ –º–æ–¥–µ–ª–∏ –∏ –¥–∞–Ω–Ω—ã–º
WEIGHTS_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\best_acc_weights.pth"
CONFIG_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\config.json"
CHARSET_PATH = r"C:\Users\USER\Desktop\trba_exp_lite\charset.txt"

# –î–∞—Ç–∞—Å–µ—Ç—ã –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
datasets = [
    {
        "image_dir": r"C:\shared\orig_cyrillic\test",
        "gt_path": r"C:\shared\orig_cyrillic\test.csv",
    },
    {
        "image_dir": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val",
        "gt_path": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val_converted.csv",
    },
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\labels.csv",
    },
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\labels.csv",
    },
]


model_path = r"C:\Users\USER\Desktop\trba_exp_lite\trba_exp_lite.onnx"
config_path = r"C:\Users\USER\Desktop\trba_exp_lite\config.json"

batch_size = 64

# === –ß–∏—Ç–∞–µ–º GT-—Ñ–∞–π–ª—ã –∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ===
gt_data = {}
total_gt_lines = 0

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    gt_path = dataset["gt_path"]
    
    print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç {idx}: {os.path.basename(image_dir)}")
    
    dataset_gt = {}
    with open(gt_path, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2:
                fname = row[0].strip()
                text = ",".join(row[1:]).strip()  # –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∑–∞–ø—è—Ç—ã–µ
                dataset_gt[fname] = text
    
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset_gt)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {os.path.basename(gt_path)}")
    total_gt_lines += len(dataset_gt)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    for fname, text in dataset_gt.items():
        if fname in gt_data:
            print(f"   ‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç —Ñ–∞–π–ª–∞: {fname} (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)")
        gt_data[fname] = text

print(f"\nüìÑ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_gt_lines} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤(–∞)")
print(f"üìÑ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(gt_data)}")

# === –°–∫–∞–Ω–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ===
valid_ext = {".png", ".jpg", ".jpeg", ".tif", ".tiff", ".bmp"}
images = []

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    
    dataset_images = [
        os.path.join(image_dir, f)
        for f in os.listdir(image_dir)
        if os.path.splitext(f)[1].lower() in valid_ext
    ]
    
    if not dataset_images:
        print(f"‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç {idx}: –í –ø–∞–ø–∫–µ {image_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!")
    else:
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç {idx}: –ù–∞–π–¥–µ–Ω–æ {len(dataset_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        images.extend(dataset_images)

# === –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
max_images = 100000000000000
if len(images) > max_images:
    print(f"‚ö†Ô∏è –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {max_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {len(images)}")
    images = images[:max_images]

if not images:
    raise RuntimeError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏ –≤ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ!")

print(f"\nüìÅ –ò–¢–û–ì–û: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
recognizer = TRBA(weights_path=model_path, config_path=config_path)

# === –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è ===
# –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã: "greedy", "beam"

# === –†–∞—Å–ø–æ–∑–Ω–∞—ë–º ===
start_time = time.perf_counter()
results = recognizer.predict(images=images, batch_size=batch_size)
end_time = time.perf_counter()
print(results)
total_time = end_time - start_time
avg_time = total_time / len(images)
fps = 1.0 / avg_time if avg_time > 0 else float("inf")

# ============================================
# –§–£–ù–ö–¶–ò–Ø –î–õ–Ø –§–ò–õ–¨–¢–†–ê–¶–ò–ò –¢–û–õ–¨–ö–û –ë–£–ö–í –ò –¶–ò–§–†
# ============================================

def filter_chars_only(text):
    """
    –û—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã (–≤–∫–ª—é—á–∞—è –∫–∏—Ä–∏–ª–ª–∏—Ü—É –∏ –¥–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ) –∏ —Ü–∏—Ñ—Ä—ã.
    –£–±–∏—Ä–∞–µ—Ç –ø—Ä–æ–±–µ–ª—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª—ã.
    """
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –¥–æ–ø—É—Å—Ç–∏–º—ã–µ —Å–∏–º–≤–æ–ª—ã: –±—É–∫–≤—ã –ª–∞—Ç–∏–Ω–∏—Ü—ã, –∫–∏—Ä–∏–ª–ª–∏—Ü—ã (–≤–∫–ª—é—á–∞—è –¥–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ) –∏ —Ü–∏—Ñ—Ä—ã
    allowed_chars = set(
        'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'
        '–∞–±–≤–≥–¥–µ—ë–∂–∑–∏–π–∫–ª–º–Ω–æ–ø—Ä—Å—Ç—É—Ñ—Ö—Ü—á—à—â—ä—ã—å—ç—é—è–ê–ë–í–ì–î–ï–Å–ñ–ó–ò–ô–ö–õ–ú–ù–û–ü–†–°–¢–£–§–•–¶–ß–®–©–™–´–¨–≠–Æ–Ø'
        '—£—¢—ñ–Ü—≥—≤—µ—¥—´—™—≠—¨—Ø—Æ—±—∞—°—†—ï—ï—ß—¶—©—®'  # –î–æ—Ä–µ—Ñ–æ—Ä–º–µ–Ω–Ω—ã–µ —Å–∏–º–≤–æ–ª—ã
        '0123456789'
    )
    return ''.join(c for c in text if c in allowed_chars)

# === –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å ground truth ===
refs, hyps = [], []
total_cer, total_wer = 0.0, 0.0
cer_count, wer_count = 0, 0
error_details = []  # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫

# –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –∏–º–µ–Ω–∞ –¥–ª—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç–∏
dataset_mapping = {}  # –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é -> —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset_results = {}  # —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è -> –º–µ—Ç—Ä–∏–∫–∏

for idx, dataset in enumerate(datasets, 1):
    # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è: –µ—Å–ª–∏ –±–∞–∑–æ–≤–æ–µ –∏–º—è –Ω–µ —É–Ω–∏–∫–∞–ª—å–Ω–æ, –¥–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–æ–≥–æ –ø—É—Ç–∏
    base_name = os.path.basename(dataset["image_dir"])
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç—å –±–∞–∑–æ–≤–æ–≥–æ –∏–º–µ–Ω–∏
    existing_names = [name for name in dataset_results.keys()]
    if base_name in existing_names:
        # –î–æ–±–∞–≤–ª—è–µ–º —á–∞—Å—Ç—å –ø—É—Ç–∏ –¥–ª—è —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
        parent = os.path.basename(os.path.dirname(dataset["image_dir"]))
        dataset_name = f"{parent}_{base_name}"
    else:
        dataset_name = base_name
    
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –∫ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è —Ç–æ—á–Ω–æ–≥–æ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏—è
    normalized_dir = os.path.normpath(dataset["image_dir"])
    dataset_mapping[normalized_dir] = dataset_name
    
    dataset_results[dataset_name] = {
        'refs': [],
        'hyps': [],
        'total_cer': 0.0,
        'total_wer': 0.0,
        'count': 0
    }

print("\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è ===")
for path, result in zip(images, results):
    pred_text = result["text"]
    score = result["confidence"]
    fname = os.path.basename(path)
    ref_text = gt_data.get(fname)
    if ref_text is None:
        print(f"{fname:40s} ‚Üí {pred_text:20s} (no GT)")
        continue

    refs.append(ref_text)
    hyps.append(pred_text)

    cer = character_error_rate(ref_text, pred_text)
    wer = word_error_rate(ref_text, pred_text)

    total_cer += cer
    total_wer += wer
    cer_count += 1
    wer_count += 1
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫ –∫–∞–∫–æ–º—É –¥–∞—Ç–∞—Å–µ—Ç—É –æ—Ç–Ω–æ—Å–∏—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
    # –ù–æ—Ä–º–∞–ª–∏–∑—É–µ–º –ø—É—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –∏—â–µ–º —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
    normalized_path = os.path.normpath(path)
    path_dir = os.path.dirname(normalized_path)
    
    dataset_name = dataset_mapping.get(path_dir)
    if dataset_name:
        dataset_results[dataset_name]['refs'].append(ref_text)
        dataset_results[dataset_name]['hyps'].append(pred_text)
        dataset_results[dataset_name]['total_cer'] += cer
        dataset_results[dataset_name]['total_wer'] += wer
        dataset_results[dataset_name]['count'] += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if ref_text != pred_text:
        error_details.append({
            'fname': fname,
            'ref': ref_text,
            'hyp': pred_text,
            'cer': cer,
            'wer': wer,
            'confidence': score
        })

    print(
        f"{fname:40s} ‚Üí {pred_text:20s} | GT: {ref_text:20s} | CER={cer:.3f} | WER={wer:.3f}"
    )

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
acc = compute_accuracy(refs, hyps)

# –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (case-insensitive)
acc_case_insensitive = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower()) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ –±—É–∫–≤–∞–º –∏ —Ü–∏—Ñ—Ä–∞–º (chars only - –±–µ–∑ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤)
refs_chars_only = [filter_chars_only(r) for r in refs]
hyps_chars_only = [filter_chars_only(h) for h in hyps]
acc_chars_only = sum(1 for r, h in zip(refs_chars_only, hyps_chars_only) if r.lower() == h.lower()) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º —Ç–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞ (–∫–æ–≥–¥–∞ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –≤–µ—Ä–Ω—ã, –Ω–æ —Ä–µ–≥–∏—Å—Ç—Ä –¥—Ä—É–≥–æ–π)
case_only_errors = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower() and r != h)

avg_cer = total_cer / max(cer_count, 1)
avg_wer = total_wer / max(wer_count, 1)

# CER/WER –¥–ª—è chars only (–ø—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—ã —Å –ø—É—Å—Ç—ã–º–∏ —Å—Ç—Ä–æ–∫–∞–º–∏)
total_cer_chars_only = 0.0
total_wer_chars_only = 0.0
chars_only_count = 0

for r, h in zip(refs_chars_only, hyps_chars_only):
    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –ø–∞—Ä—ã –≥–¥–µ —Ö–æ—Ç—è –±—ã –æ–¥–Ω–∞ —Å—Ç—Ä–æ–∫–∞ –ø—É—Å—Ç–∞—è
    if not r and not h:
        # –û–±–µ –ø—É—Å—Ç—ã–µ - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (CER=0, WER=0)
        chars_only_count += 1
        continue
    elif not r or not h:
        # –û–¥–Ω–∞ –ø—É—Å—Ç–∞—è, –¥—Ä—É–≥–∞—è –Ω–µ—Ç - —Å—á–∏—Ç–∞–µ–º –∫–∞–∫ –ø–æ–ª–Ω—É—é –æ—à–∏–±–∫—É
        total_cer_chars_only += 1.0
        total_wer_chars_only += 1.0
        chars_only_count += 1
        continue
    
    # –û–±–µ –Ω–µ–ø—É—Å—Ç—ã–µ - —Å—á–∏—Ç–∞–µ–º –Ω–æ—Ä–º–∞–ª—å–Ω–æ
    total_cer_chars_only += character_error_rate(r, h)
    total_wer_chars_only += word_error_rate(r, h)
    chars_only_count += 1

avg_cer_chars_only = total_cer_chars_only / max(chars_only_count, 1)
avg_wer_chars_only = total_wer_chars_only / max(chars_only_count, 1)

print("\n=== –°–≤–æ–¥–∫–∞ ===")
print(f"Accuracy (case-sensitive):     {acc*100:.2f}%")
print(f"Accuracy (case-insensitive):   {acc_case_insensitive*100:.2f}%")
print(f"Accuracy (chars only):         {acc_chars_only*100:.2f}%")
print(f"Case-only errors:              {case_only_errors} ({case_only_errors/max(len(refs), 1)*100:.2f}%)")
print(f"Avg CER:  {avg_cer:.4f}")
print(f"Avg WER:  {avg_wer:.4f}")
print(f"Processed {len(images)} images in {total_time:.3f} sec")
print(f"Average per image: {avg_time:.3f} sec ({fps:.1f} FPS)")

# ============================================
# –¢–ê–ë–õ–ò–¶–ê –ú–ï–¢–†–ò–ö –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú
# ============================================

print("\n" + "="*100)
print("üìä –ú–ï–¢–†–ò–ö–ò –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú")
print("="*100)

# –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
metrics_table = []

for dataset_name, data in dataset_results.items():
    if data['count'] == 0:
        continue
    
    # Accuracy (case-sensitive)
    acc_ds = compute_accuracy(data['refs'], data['hyps'])
    
    # Accuracy (case-insensitive)
    acc_ci_ds = sum(1 for r, h in zip(data['refs'], data['hyps']) if r.lower() == h.lower()) / max(data['count'], 1)
    
    # Accuracy (chars only)
    refs_co_ds = [filter_chars_only(r) for r in data['refs']]
    hyps_co_ds = [filter_chars_only(h) for h in data['hyps']]
    acc_co_ds = sum(1 for r, h in zip(refs_co_ds, hyps_co_ds) if r.lower() == h.lower()) / max(data['count'], 1)
    
    # CER/WER
    avg_cer_ds = data['total_cer'] / data['count']
    avg_wer_ds = data['total_wer'] / data['count']
    
    # CER/WER (case-insensitive)
    cer_ci_ds = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(data['refs'], data['hyps'])) / data['count']
    wer_ci_ds = sum(word_error_rate(r.lower(), h.lower()) for r, h in zip(data['refs'], data['hyps'])) / data['count']
    
    # CER/WER (chars only) - —Å –æ–±—Ä–∞–±–æ—Ç–∫–æ–π –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
    total_cer_co_ds = 0.0
    total_wer_co_ds = 0.0
    co_count_ds = 0
    
    for r, h in zip(refs_co_ds, hyps_co_ds):
        if not r and not h:
            co_count_ds += 1
            continue
        elif not r or not h:
            total_cer_co_ds += 1.0
            total_wer_co_ds += 1.0
            co_count_ds += 1
            continue
        total_cer_co_ds += character_error_rate(r, h)
        total_wer_co_ds += word_error_rate(r, h)
        co_count_ds += 1
    
    cer_co_ds = total_cer_co_ds / max(co_count_ds, 1)
    wer_co_ds = total_wer_co_ds / max(co_count_ds, 1)
    
    metrics_table.append({
        'Dataset': dataset_name,
        'Count': data['count'],
        'Acc (CS)': acc_ds,
        'Acc (CI)': acc_ci_ds,
        'Acc (CO)': acc_co_ds,
        'CER (CS)': avg_cer_ds,
        'CER (CI)': cer_ci_ds,
        'CER (CO)': cer_co_ds,
        'WER (CS)': avg_wer_ds,
        'WER (CI)': wer_ci_ds,
        'WER (CO)': wer_co_ds,
    })

# –î–æ–±–∞–≤–ª—è–µ–º –æ–±—â—É—é —Å—Ç—Ä–æ–∫—É (TOTAL)
# Case-insensitive –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö
total_cer_ci = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps)) / max(len(refs), 1)
total_wer_ci = sum(word_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps)) / max(len(refs), 1)

metrics_table.append({
    'Dataset': 'TOTAL',
    'Count': len(refs),
    'Acc (CS)': acc,
    'Acc (CI)': acc_case_insensitive,
    'Acc (CO)': acc_chars_only,
    'CER (CS)': avg_cer,
    'CER (CI)': total_cer_ci,
    'CER (CO)': avg_cer_chars_only,
    'WER (CS)': avg_wer,
    'WER (CI)': total_wer_ci,
    'WER (CO)': avg_wer_chars_only,
})

# –í—ã–≤–æ–¥ —Ç–∞–±–ª–∏—Ü—ã
print(f"\n{'Dataset':<30} {'Count':>6} {'Acc(CS)':>8} {'Acc(CI)':>8} {'Acc(CO)':>8} {'CER(CS)':>8} {'CER(CI)':>8} {'CER(CO)':>8} {'WER(CS)':>8} {'WER(CI)':>8} {'WER(CO)':>8}")
print("-" * 100)
for row in metrics_table:
    is_total = row['Dataset'] == 'TOTAL'
    sep = "=" if is_total else "-"
    if is_total:
        print(sep * 100)
    print(f"{row['Dataset']:<30} {row['Count']:>6} {row['Acc (CS)']*100:>7.2f}% {row['Acc (CI)']*100:>7.2f}% {row['Acc (CO)']*100:>7.2f}% "
          f"{row['CER (CS)']:>8.4f} {row['CER (CI)']:>8.4f} {row['CER (CO)']:>8.4f} "
          f"{row['WER (CS)']:>8.4f} {row['WER (CI)']:>8.4f} {row['WER (CO)']:>8.4f}")

# –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ç–∞–±–ª–∏—Ü—ã –≤ CSV
csv_output_path = os.path.join(os.path.dirname(model_path), "metrics_by_dataset.csv")
with open(csv_output_path, "w", newline="", encoding="utf-8") as csvfile:
    writer = csv.DictWriter(csvfile, fieldnames=[
        'Dataset', 'Count', 
        'Acc (CS)', 'Acc (CI)', 'Acc (CO)',
        'CER (CS)', 'CER (CI)', 'CER (CO)',
        'WER (CS)', 'WER (CI)', 'WER (CO)'
    ])
    writer.writeheader()
    for row in metrics_table:
        writer.writerow(row)

print(f"\nüíæ –¢–∞–±–ª–∏—Ü–∞ –º–µ—Ç—Ä–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: {csv_output_path}")
print("="*100)

# ============================================
# –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
# ============================================

def analyze_character_errors(refs, hyps):
    """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    
    substitutions = Counter()  # (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –æ—à–∏–±–æ—á–Ω—ã–π)
    insertions = Counter()     # –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    deletions = Counter()      # —É–¥–∞–ª—ë–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    
    error_positions = {'start': 0, 'middle': 0, 'end': 0}  # –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫
    
    for ref, hyp in zip(refs, hyps):
        if ref == hyp:
            continue
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ Levenshtein –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        ops = Levenshtein.editops(ref, hyp)
        
        for op_type, ref_pos, hyp_pos in ops:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –æ—à–∏–±–∫–∏ –≤ —Å–ª–æ–≤–µ
            word_len = len(ref)
            if ref_pos < word_len * 0.2:
                error_positions['start'] += 1
            elif ref_pos > word_len * 0.8:
                error_positions['end'] += 1
            else:
                error_positions['middle'] += 1
            
            if op_type == 'replace':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                substitutions[(ref_char, hyp_char)] += 1
            elif op_type == 'insert':
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                insertions[hyp_char] += 1
            elif op_type == 'delete':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                deletions[ref_char] += 1
    
    return substitutions, insertions, deletions, error_positions


def analyze_word_lengths(error_details):
    """–ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏"""
    error_lengths = []
    correct_lengths = []
    
    for detail in error_details:
        error_lengths.append(len(detail['ref']))
    
    return error_lengths


def analyze_error_types(error_details):
    """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫"""
    
    total_errors = len(error_details)
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—à–∏–±–æ–∫
    length_mismatch = 0
    case_errors = 0
    similar_chars = 0
    completely_wrong = 0
    
    for detail in error_details:
        ref = detail['ref']
        hyp = detail['hyp']
        
        if len(ref) != len(hyp):
            length_mismatch += 1
        elif ref.lower() == hyp.lower():
            case_errors += 1
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            distance = Levenshtein.distance(ref, hyp)
            if distance <= 2:
                similar_chars += 1
            else:
                completely_wrong += 1
    
    return {
        'total': total_errors,
        'length_mismatch': length_mismatch,
        'case_errors': case_errors,
        'similar_chars': similar_chars,
        'completely_wrong': completely_wrong
    }


if error_details:
    print("\n" + "="*80)
    print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö")
    print("="*80)
    
    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
    print(f"\n1Ô∏è‚É£ –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(refs)}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(refs) - len(error_details)}")
    print(f"   –° –æ—à–∏–±–∫–∞–º–∏: {len(error_details)} ({len(error_details)/len(refs)*100:.1f}%)")
    
    # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏ (—É–∂–µ –ø–æ—Å—á–∏—Ç–∞–Ω—ã –≤—ã—à–µ)
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-sensitive):")
    print(f"      Accuracy: {acc*100:.2f}%")
    print(f"      CER: {avg_cer:.4f}")
    print(f"      WER: {avg_wer:.4f}")
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-insensitive):")
    print(f"      Accuracy: {acc_case_insensitive*100:.2f}%")
    print(f"      CER: {total_cer_ci:.4f}")
    print(f"      WER: {total_wer_ci:.4f}")
    if avg_cer > 0:
        print(f"      –£–ª—É—á—à–µ–Ω–∏–µ CER: {(avg_cer - total_cer_ci)/avg_cer*100:.1f}%")
    if avg_wer > 0:
        print(f"      –£–ª—É—á—à–µ–Ω–∏–µ WER: {(avg_wer - total_wer_ci)/avg_wer*100:.1f}%")
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (chars only - –±–µ–∑ —Å–ø–µ—Ü—Å–∏–º–≤–æ–ª–æ–≤ –∏ –ø—Ä–æ–±–µ–ª–æ–≤):")
    print(f"      Accuracy: {acc_chars_only*100:.2f}%")
    print(f"      CER: {avg_cer_chars_only:.4f}")
    print(f"      WER: {avg_wer_chars_only:.4f}")
    if avg_cer > 0:
        print(f"      –£–ª—É—á—à–µ–Ω–∏–µ CER: {(avg_cer - avg_cer_chars_only)/avg_cer*100:.1f}%")
    if avg_wer > 0:
        print(f"      –£–ª—É—á—à–µ–Ω–∏–µ WER: {(avg_wer - avg_wer_chars_only)/avg_wer*100:.1f}%")
    
    # 2. –¢–∏–ø—ã –æ—à–∏–±–æ–∫
    print(f"\n2Ô∏è‚É£ –¢–ò–ü–´ –û–®–ò–ë–û–ö:")
    error_types = analyze_error_types(error_details)
    print(f"   –†–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞: {error_types['length_mismatch']} ({error_types['length_mismatch']/error_types['total']*100:.1f}%)")
    print(f"   –¢–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä: {error_types['case_errors']} ({error_types['case_errors']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ—Ö–æ–∂–∏–µ (1-2 —Å–∏–º–≤–æ–ª–∞): {error_types['similar_chars']} ({error_types['similar_chars']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–µ—Ä–Ω—ã–µ: {error_types['completely_wrong']} ({error_types['completely_wrong']/error_types['total']*100:.1f}%)")
    
    # 3. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    print(f"\n3Ô∏è‚É£ –î–õ–ò–ù–ê –°–õ–û–í –° –û–®–ò–ë–ö–ê–ú–ò:")
    error_lengths = analyze_word_lengths(error_details)
    if error_lengths:
        avg_error_len = sum(error_lengths) / len(error_lengths)
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_error_len:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∏–Ω: {min(error_lengths)}, –ú–∞–∫—Å: {max(error_lengths)}")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º
        length_dist = Counter(error_lengths)
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for length in sorted(length_dist.keys())[:10]:  # –¢–æ–ø-10
            print(f"      {length} —Å–∏–º–≤–æ–ª–æ–≤: {length_dist[length]} —Å–ª–æ–≤")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    print(f"\n4Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ü–û –°–ò–ú–í–û–õ–ê–ú:")
    substitutions, insertions, deletions, error_positions = analyze_character_errors(refs, hyps)
    
    # –ü–æ–∑–∏—Ü–∏–∏ –æ—à–∏–±–æ–∫
    total_pos = sum(error_positions.values())
    if total_pos > 0:
        print(f"   –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ —Å–ª–æ–≤–µ:")
        print(f"      –ù–∞—á–∞–ª–æ (0-20%): {error_positions['start']} ({error_positions['start']/total_pos*100:.1f}%)")
        print(f"      –°–µ—Ä–µ–¥–∏–Ω–∞ (20-80%): {error_positions['middle']} ({error_positions['middle']/total_pos*100:.1f}%)")
        print(f"      –ö–æ–Ω–µ—Ü (80-100%): {error_positions['end']} ({error_positions['end']/total_pos*100:.1f}%)")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã
    print(f"\n   üîÑ –¢–æ–ø-20 –∑–∞–º–µ–Ω —Å–∏–º–≤–æ–ª–æ–≤ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ‚Üí –æ—à–∏–±–æ—á–Ω—ã–π):")
    
    # –†–∞–∑–¥–µ–ª–∏–º –∑–∞–º–µ–Ω—ã –Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∏ –Ω–µ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ
    case_substitutions = []
    non_case_substitutions = []
    
    for (correct, wrong), count in substitutions.items():
        if correct.lower() == wrong.lower() and correct != wrong:
            case_substitutions.append(((correct, wrong), count))
        else:
            non_case_substitutions.append(((correct, wrong), count))
    
    case_substitutions.sort(key=lambda x: x[1], reverse=True)
    non_case_substitutions.sort(key=lambda x: x[1], reverse=True)
    
    if case_substitutions:
        print(f"\n      –†–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-10):")
        for (correct, wrong), count in case_substitutions[:10]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    if non_case_substitutions:
        print(f"\n      –î—Ä—É–≥–∏–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-20):")
        for (correct, wrong), count in non_case_substitutions[:20]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤—Å—Ç–∞–≤–∫–∏
    if insertions:
        print(f"\n   ‚ûï –¢–æ–ø-10 –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in insertions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —É–¥–∞–ª–µ–Ω–∏—è
    if deletions:
        print(f"\n   ‚ûñ –¢–æ–ø-10 –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in deletions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # 5. –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\n5Ô∏è‚É£ –•–£–î–®–ò–ï –ü–†–ò–ú–ï–†–´ (—Ç–æ–ø-10 –ø–æ CER):")
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:10]
    for i, ex in enumerate(worst_examples, 1):
        print(f"   {i}. [{ex['fname']}]")
        print(f"      GT:   '{ex['ref']}'")
        print(f"      Pred: '{ex['hyp']}'")
        print(f"      CER: {ex['cer']:.3f}, Conf: {ex['confidence']:.3f}")

    # === 5. –í–°–ï –û–®–ò–ë–ö–ò (—Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ 4 HTML, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ GT) ===
    print(f"\n5Ô∏è‚É£ –°–û–ó–î–ê–Å–ú HTML-–û–¢–ß–Å–¢–´ –°–û –í–°–ï–ú–ò –û–®–ò–ë–ö–ê–ú–ò (—Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ 4 —á–∞—Å—Ç–∏, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ GT)...")

    import base64
    from io import BytesIO
    from PIL import Image
    import math

    # === 1. –ë–µ—Ä—ë–º –≤—Å–µ –æ—à–∏–±–∫–∏, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ GT ===
    all_errors = sorted(error_details, key=lambda x: x['ref'].lower())
    num_errors = len(all_errors)
    num_parts = 4
    part_size = math.ceil(num_errors / num_parts)

    print(f"   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {num_errors}")
    print(f"   –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ {num_parts} HTML-—Ñ–∞–π–ª–∞ –ø–æ ~{part_size} –∑–∞–ø–∏—Å–µ–π –∫–∞–∂–¥—ã–π")

    # === 2. –û–±—â–∏–π —Å—Ç–∏–ª—å –∏ JS (–µ–¥–∏–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π) ===
    def make_html_header(title):
        return [
            "<html><head><meta charset='utf-8'>",
            "<style>",
            "body { font-family: Arial, sans-serif; background: #fafafa; }",
            "table { border-collapse: collapse; width: 100%; margin: 20px 0; table-layout: fixed; }",
            "th, td { border: 1px solid #ccc; padding: 6px 10px; text-align: left; vertical-align: middle; overflow-wrap: break-word; }",
            "th { background-color: #f2f2f2; }",
            "td:nth-child(2) { width: 150px; text-align: center; }",
            "img { max-width: 140px; max-height: 80px; object-fit: contain; border-radius: 6px; background: #fff; }",
            ".gt { color: #006400; font-weight: bold; }",
            ".pred { color: #8B0000; font-weight: bold; }",
            ".edit { background: #ffffe0; }",
            ".num { text-align: center; }",
            "button { margin: 10px; padding: 6px 10px; }",
            "</style></head><body>",
            f"<h2>{title}</h2>",
            "<div>",
            "<button onclick='resizeImages(0.5)'>üîç –£–º–µ–Ω—å—à–∏—Ç—å</button>",
            "<button onclick='resizeImages(1)'>üîé –ù–æ—Ä–º–∞–ª—å–Ω–æ</button>",
            "<button onclick='resizeImages(2)'>üîç –£–≤–µ–ª–∏—á–∏—Ç—å</button>",
            "<button onclick='downloadCorrections()'>üíæ –°–∫–∞—á–∞—Ç—å –ø—Ä–∞–≤–∫–∏ (CSV)</button>",
            "</div>",
            "<script>",
            "function resizeImages(scale){document.querySelectorAll('img').forEach(img=>{img.style.maxWidth=(140*scale)+'px';img.style.maxHeight=(80*scale)+'px';});}",
            "function saveCorrection(id){const val=document.getElementById('edit_'+id).innerText.trim();localStorage.setItem('ocr_edit_'+id,val);}",
            "function loadCorrections(){document.querySelectorAll('[id^=edit_]').forEach(el=>{const saved=localStorage.getItem('ocr_edit_'+el.id.split('edit_')[1]);if(saved){el.innerText=saved;}});}",
            "function downloadCorrections(){let rows=[['#','filename','GT','Pred','CER','Conf','Correction']];document.querySelectorAll('tr[data-id]').forEach(tr=>{const id=tr.getAttribute('data-id');const cells=tr.querySelectorAll('td');const correction=document.getElementById('edit_'+id).innerText.trim().replace(/\\n/g,' ');rows.push([id,cells[2].innerText,cells[3].innerText,cells[4].innerText,cells[5].innerText,cells[6].innerText,correction]);});const csvContent=rows.map(r=>r.map(v=>'\"'+v.replaceAll('\"','\"\"')+'\"').join(',')).join('\\n');const blob=new Blob([csvContent],{type:'text/csv;charset=utf-8;'});const a=document.createElement('a');a.href=URL.createObjectURL(blob);a.download='ocr_corrections.csv';a.click();}",
            "window.onload=loadCorrections;",
            "</script>",
            "<table>",
            "<tr><th>#</th><th>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</th><th>–§–∞–π–ª</th><th>GT</th><th>Pred</th><th>CER</th><th>Conf.</th><th>–ü—Ä–∞–≤–∫–∞ ‚úèÔ∏è</th></tr>"
        ]

    def make_html_row(i, ex):
        fname = ex['fname']
        cer = f"{ex['cer']:.3f}"
        conf = f"{ex['confidence']:.3f}"
        gt = ex['ref'].replace("<", "&lt;").replace(">", "&gt;")
        pred = ex['hyp'].replace("<", "&lt;").replace(">", "&gt;")

        # –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        img_path = None
        for d in datasets:
            candidate = os.path.join(d["image_dir"], fname)
            if os.path.exists(candidate):
                img_path = candidate
                break

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        if img_path:
            try:
                with Image.open(img_path) as img:
                    img.thumbnail((400, 200))
                    buffer = BytesIO()
                    img.save(buffer, format="JPEG", quality=80)
                    img_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
                    img_tag = f"<img src='data:image/jpeg;base64,{img_base64}'>"
            except Exception:
                img_tag = f"<div style='color:red;'>–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏</div>"
        else:
            img_tag = "<div style='color:gray;'>–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</div>"

        return (
            f"<tr data-id='{i}'>"
            f"<td class='num'>{i}</td>"
            f"<td>{img_tag}</td>"
            f"<td>{fname}</td>"
            f"<td class='gt'>{gt}</td>"
            f"<td class='pred'>{pred}</td>"
            f"<td class='num'>{cer}</td>"
            f"<td class='num'>{conf}</td>"
            f"<td class='edit' id='edit_{i}' contenteditable='true' oninput='saveCorrection({i})'></td>"
            f"</tr>"
        )

    # === 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 4 HTML-—Ñ–∞–π–ª–æ–≤ ===
    for part_idx in range(num_parts):
        start = part_idx * part_size
        end = min(start + part_size, num_errors)
        subset = all_errors[start:end]

        if not subset:
            continue

        html_lines = make_html_header(f"üìä OCR –æ—à–∏–±–∫–∏ (—á–∞—Å—Ç—å {part_idx+1} –∏–∑ {num_parts}) ‚Äî –∑–∞–ø–∏—Å–∏ {start+1}‚Äì{end}")
        for i, ex in enumerate(subset, start + 1):
            html_lines.append(make_html_row(i, ex))
        html_lines.append("</table></body></html>")

        html_path = os.path.join(
            os.path.dirname(model_path),
            f"ocr_all_errors_part{part_idx+1}.html"
        )

        with open(html_path, "w", encoding="utf-8") as f:
            f.write("\n".join(html_lines))

        print(f"üíæ HTML-–æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_path}")

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {num_parts} HTML-—Ñ–∞–π–ª–æ–≤ —Å–æ –≤—Å–µ–º–∏ {num_errors} –æ—à–∏–±–∫–∞–º–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ GT).")


    # 6. –°–≤—è–∑—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–æ–∫
    print(f"\n6Ô∏è‚É£ –°–í–Ø–ó–¨ –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ò –û–®–ò–ë–û–ö:")
    low_conf_errors = [e for e in error_details if e['confidence'] < 0.8]
    high_conf_errors = [e for e in error_details if e['confidence'] >= 0.8]
    print(f"   –û—à–∏–±–∫–∏ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (<0.8): {len(low_conf_errors)} ({len(low_conf_errors)/len(error_details)*100:.1f}%)")
    print(f"   –û—à–∏–±–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (‚â•0.8): {len(high_conf_errors)} ({len(high_conf_errors)/len(error_details)*100:.1f}%)")
    
    if error_details:
        avg_conf_errors = sum(e['confidence'] for e in error_details) / len(error_details)
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—à–∏–±–∫–∞—Ö: {avg_conf_errors:.3f}")
   
    # 7. –í—Å–µ –æ—à–∏–±–∫–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    print(f"\n7Ô∏è‚É£ –í–°–ï –û–®–ò–ë–ö–ò (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏):")
    sorted_errors = sorted(error_details, key=lambda x: x['confidence'], reverse=True)
    
    print(f"{'–§–∞–π–ª':30s} | {'Conf.':>7s} | {'CER':>5s} | {'GT':25s} | {'Pred':25s}")
    print("-" * 100)
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ CSV ===
    import csv

    output_csv = os.path.join(os.path.dirname(model_path), "ocr_errors_by_confidence.csv")
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["filename", "confidence", "CER", "WER", "GT", "Prediction"])
        for err in sorted_errors:
            writer.writerow([
                err['fname'],
                f"{err['confidence']:.4f}",
                f"{err['cer']:.4f}",
                f"{err['wer']:.4f}",
                err['ref'],
                err['hyp'],
            ])

    print(f"\nüíæ –û—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_csv}")
else:
    print("\n‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫! –í—Å–µ —Å–ª–æ–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã –∏–¥–µ–∞–ª—å–Ω–æ!")

print("\n" + "="*80)