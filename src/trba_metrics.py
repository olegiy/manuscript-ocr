import os
import time
import csv
import re
from collections import Counter, defaultdict
from manuscript.recognizers import TRBA
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    word_error_rate,
    compute_accuracy,
)
from manuscript.recognizers._trba.data.dataset import OCRDatasetAttn
from manuscript.recognizers._trba.data.transforms import load_charset
import Levenshtein
from tqdm import tqdm


def normalize_text_letters_only(text: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ç–µ–∫—Å—Ç: –æ—Å—Ç–∞–≤–ª—è–µ—Ç —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã, –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ –Ω–∏–∂–Ω–µ–º—É —Ä–µ–≥–∏—Å—Ç—Ä—É.
    –£–¥–∞–ª—è–µ—Ç –ø—Ä–æ–±–µ–ª—ã, –ø—É–Ω–∫—Ç—É–∞—Ü–∏—é –∏ —Ü–∏—Ñ—Ä—ã.
    """
    # –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –±—É–∫–≤—ã (–ª–∞—Ç–∏–Ω–∏—Ü–∞ + –∫–∏—Ä–∏–ª–ª–∏—Ü–∞ + –¥—Ä. Unicode –±—É–∫–≤—ã)
    letters_only = re.sub(r'[^a-zA-Z–∞-—è–ê-–Ø—ë–Å\u0080-\uFFFF]', '', text)
    return letters_only.lower()


# === –ü—É—Ç–∏ ===
# –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ - –æ–Ω–∏ –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã
datasets = [
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\labels.csv",
    },
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\labels.csv",
    },
    {
        "image_dir": r"C:\shared\orig_cyrillic\test",
        "gt_path": r"C:\shared\orig_cyrillic\test.tsv",  # –¢–æ—Ç –∂–µ —Ñ–∞–π–ª —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    },
    {
        "image_dir": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val",
        "gt_path": r"C:\shared\school_notebooks_RU\school_notebooks_RU\val_converted.csv",
    },
]

model_path = r"C:\Users\USER\Desktop\trba_exp_lite\trba_exp_lite.onnx"
config_path = r"C:\Users\USER\Desktop\trba_exp_lite\config.json"
charset_path = r"C:\Users\USER\Desktop\trba_exp_lite\charset.txt"

batch_size = 64

# === –ó–∞–≥—Ä—É–∂–∞–µ–º charset –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ===
print("üìö –ó–∞–≥—Ä—É–∑–∫–∞ charset...")
itos, stoi = load_charset(charset_path)
print(f"   –†–∞–∑–º–µ—Ä –∞–ª—Ñ–∞–≤–∏—Ç–∞: {len(itos)} —Å–∏–º–≤–æ–ª–æ–≤")

# === –ß–∏—Ç–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏—Å–ø–æ–ª—å–∑—É—è —Ç–æ—Ç –∂–µ OCRDatasetAttn —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ ===
all_samples = []  # –°–ø–∏—Å–æ–∫ –∫–æ—Ä—Ç–µ–∂–µ–π (image_path, label, dataset_idx)
image_to_dataset = {}  # –ú–∞–ø–ø–∏–Ω–≥: –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é ‚Üí –∏–Ω–¥–µ–∫—Å –¥–∞—Ç–∞—Å–µ—Ç–∞

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    gt_path = dataset["gt_path"]
    
    print(f"\nüìÇ –î–∞—Ç–∞—Å–µ—Ç {idx}: {os.path.basename(image_dir)}")
    print(f"   CSV: {os.path.basename(gt_path)}")
    
    # –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç —Å –¢–û–ß–ù–û –¢–ï–ú–ò –ñ–ï –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏ —á—Ç–æ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏!
    # –í–ê–ñ–ù–û: strict_max_len=True –∏ max_len=40 –∫–∞–∫ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏
    ds = OCRDatasetAttn(
        csv_path=gt_path,
        images_dir=image_dir,
        stoi=stoi,
        img_height=32,
        img_max_width=256,  # –ö–∞–∫ –≤ config.json
        transform=None,
        has_header=None,  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        encoding="utf-8",
        delimiter=None,  # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
        strict_charset=False,  # –ù–µ —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∞–ª—Ñ–∞–≤–∏—Ç—É
        validate_image=False,  # –ù–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
        max_len=40,  # –ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò!
        strict_max_len=True,  # –ö–ê–ö –ü–†–ò –û–ë–£–ß–ï–ù–ò–ò! –§–∏–ª—å—Ç—Ä—É–µ–º –¥–ª–∏–Ω–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã
        num_workers=0
    )
    
    print(f"   ‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(ds)} –≤–∞–ª–∏–¥–Ω—ã—Ö –ø—Ä–∏–º–µ—Ä–æ–≤")
    
    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ (–µ—Å–ª–∏ –±—ã–ª–∏ –æ—Ç–±—Ä–æ—Å—ã)
    if hasattr(ds, '_reasons'):
        total_filtered = sum(ds._reasons.values())
        if total_filtered > 0:
            print(f"   ‚ö†Ô∏è  –û—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ {total_filtered} –ø—Ä–∏–º–µ—Ä–æ–≤:")
            for reason, count in ds._reasons.items():
                if count > 0:
                    print(f"      - {reason}: {count}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –ø—É—Ç–∏ –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º –∏ –º–µ—Ç–∫–∏
    for i in range(len(ds)):
        img_path, label = ds.samples[i]
        all_samples.append((img_path, label, idx))
        image_to_dataset[img_path] = idx

# === –§–æ—Ä–º–∏—Ä—É–µ–º —Å–ø–∏—Å–∫–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è ===
images = [sample[0] for sample in all_samples]
gt_data = {os.path.basename(sample[0]): sample[1] for sample in all_samples}

# === –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π ===
max_images = 1000000000000
if len(images) > max_images:
    print(f"\n‚ö†Ô∏è –ë–µ—Ä—ë–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–µ {max_images} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏–∑ {len(images)}")
    images = images[:max_images]
    all_samples = all_samples[:max_images]

if not images:
    raise RuntimeError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏ –≤ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ!")

print(f"\nüìÅ –ò–¢–û–ì–û: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
print(f"\nüîß –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
recognizer = TRBA(weights_path=model_path, config_path=config_path, charset_path=charset_path)

# === –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è ===
# –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã: "greedy", "beam"

# === –†–∞—Å–ø–æ–∑–Ω–∞—ë–º ===
start_time = time.perf_counter()
results = recognizer.predict(images=images, batch_size=batch_size)
end_time = time.perf_counter()
print(results)
total_time = end_time - start_time
avg_time = total_time / len(images)
fps = 1.0 / avg_time if avg_time > 0 else float("inf")

# === –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å ground truth ===
refs, hyps = [], []
dataset_ids = []  # –ò–Ω–¥–µ–∫—Å –¥–∞—Ç–∞—Å–µ—Ç–∞ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
total_cer, total_wer = 0.0, 0.0
cer_count, wer_count = 0, 0
error_details = []  # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫

print("\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è ===")
for path, result in zip(images, results):
    pred_text = result["text"]
    score = result["confidence"]
    fname = os.path.basename(path)
    ref_text = gt_data.get(fname)
    if ref_text is None:
        print(f"{fname:40s} ‚Üí {pred_text:20s} (no GT)")
        continue

    refs.append(ref_text)
    hyps.append(pred_text)
    dataset_ids.append(image_to_dataset.get(path, 0))  # –ó–∞–ø–æ–º–∏–Ω–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç

    cer = character_error_rate(ref_text, pred_text)
    wer = word_error_rate(ref_text, pred_text)

    total_cer += cer
    total_wer += wer
    cer_count += 1
    wer_count += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if ref_text != pred_text:
        error_details.append({
            'fname': fname,
            'ref': ref_text,
            'hyp': pred_text,
            'cer': cer,
            'wer': wer,
            'confidence': score,
            'dataset_id': image_to_dataset.get(path, 0)
        })

    print(
        f"{fname:40s} ‚Üí {pred_text:20s} | GT: {ref_text:20s} | CER={cer:.3f} | WER={wer:.3f}"
    )

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
acc = compute_accuracy(refs, hyps)

# –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (case-insensitive)
acc_case_insensitive = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower()) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Ç–æ–ª—å–∫–æ –ø–æ –±—É–∫–≤–∞–º (–±–µ–∑ –ø—É–Ω–∫—Ç—É–∞—Ü–∏–∏, –ø—Ä–æ–±–µ–ª–æ–≤, —Ä–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–æ)
acc_letters_only = sum(
    1 for r, h in zip(refs, hyps) 
    if normalize_text_letters_only(r) == normalize_text_letters_only(h)
) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º —Ç–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞ (–∫–æ–≥–¥–∞ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –≤–µ—Ä–Ω—ã, –Ω–æ —Ä–µ–≥–∏—Å—Ç—Ä –¥—Ä—É–≥–æ–π)
case_only_errors = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower() and r != h)

avg_cer = total_cer / max(cer_count, 1)
avg_wer = total_wer / max(wer_count, 1)

# === –í—ã—á–∏—Å–ª–µ–Ω–∏–µ –º–µ—Ç—Ä–∏–∫ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º ===
def compute_dataset_metrics(refs, hyps, dataset_ids, dataset_idx):
    """–í—ã—á–∏—Å–ª—è–µ—Ç –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä—ã –∏–∑ —ç—Ç–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    dataset_refs = [r for r, h, d in zip(refs, hyps, dataset_ids) if d == dataset_idx]
    dataset_hyps = [h for r, h, d in zip(refs, hyps, dataset_ids) if d == dataset_idx]
    
    if not dataset_refs:
        return None
    
    # –ë–∞–∑–æ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    acc = compute_accuracy(dataset_refs, dataset_hyps)
    acc_ci = sum(1 for r, h in zip(dataset_refs, dataset_hyps) if r.lower() == h.lower()) / len(dataset_refs)
    acc_letters = sum(
        1 for r, h in zip(dataset_refs, dataset_hyps) 
        if normalize_text_letters_only(r) == normalize_text_letters_only(h)
    ) / len(dataset_refs)
    
    # CER –∏ WER
    total_cer = sum(character_error_rate(r, h) for r, h in zip(dataset_refs, dataset_hyps))
    total_wer = sum(word_error_rate(r, h) for r, h in zip(dataset_refs, dataset_hyps))
    avg_cer = total_cer / len(dataset_refs)
    avg_wer = total_wer / len(dataset_refs)
    
    # CER case-insensitive
    total_cer_ci = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(dataset_refs, dataset_hyps))
    avg_cer_ci = total_cer_ci / len(dataset_refs)
    
    # CER letters only
    total_cer_letters = 0.0
    for r, h in zip(dataset_refs, dataset_hyps):
        r_letters = normalize_text_letters_only(r)
        h_letters = normalize_text_letters_only(h)
        if r_letters:
            total_cer_letters += character_error_rate(r_letters, h_letters)
    avg_cer_letters = total_cer_letters / len(dataset_refs)
    
    return {
        'count': len(dataset_refs),
        'acc': acc,
        'acc_ci': acc_ci,
        'acc_letters': acc_letters,
        'cer': avg_cer,
        'cer_ci': avg_cer_ci,
        'cer_letters': avg_cer_letters,
        'wer': avg_wer
    }

print("\n" + "="*120)
print("üìä –ú–ï–¢–†–ò–ö–ò –ü–û –î–ê–¢–ê–°–ï–¢–ê–ú")
print("="*120)

# –í—ã—á–∏—Å–ª—è–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
dataset_metrics = {}
for idx in range(1, len(datasets) + 1):
    metrics = compute_dataset_metrics(refs, hyps, dataset_ids, idx)
    if metrics:
        dataset_metrics[idx] = metrics

# –û–±—â–∏–µ –º–µ—Ç—Ä–∏–∫–∏
overall_metrics = {
    'count': len(refs),
    'acc': acc,
    'acc_ci': acc_case_insensitive,
    'acc_letters': acc_letters_only,
    'cer': avg_cer,
    'wer': avg_wer
}

# –í—ã—á–∏—Å–ª—è–µ–º –æ–±—â–∏–µ CER case-insensitive –∏ letters only –¥–ª—è —Ç–∞–±–ª–∏—Ü—ã
total_cer_ci = sum(character_error_rate(r.lower(), h.lower()) for r, h in zip(refs, hyps))
overall_metrics['cer_ci'] = total_cer_ci / max(len(refs), 1)

total_cer_letters = 0.0
for r, h in zip(refs, hyps):
    r_letters = normalize_text_letters_only(r)
    h_letters = normalize_text_letters_only(h)
    if r_letters:
        total_cer_letters += character_error_rate(r_letters, h_letters)
overall_metrics['cer_letters'] = total_cer_letters / max(len(refs), 1)

# –ü–µ—á–∞—Ç–∞–µ–º —Ç–∞–±–ª–∏—Ü—É
print(f"\n{'–î–∞—Ç–∞—Å–µ—Ç':<30} {'Count':>8} {'Acc':>8} {'Acc-CI':>8} {'Acc-L':>8} {'CER':>8} {'CER-CI':>8} {'CER-L':>8} {'WER':>8}")
print("-" * 120)

for idx in sorted(dataset_metrics.keys()):
    dataset = datasets[idx - 1]
    dataset_name = os.path.basename(dataset["image_dir"])[:28]
    m = dataset_metrics[idx]
    print(f"{dataset_name:<30} {m['count']:>8} {m['acc']*100:>7.2f}% {m['acc_ci']*100:>7.2f}% {m['acc_letters']*100:>7.2f}% {m['cer']:>8.4f} {m['cer_ci']:>8.4f} {m['cer_letters']:>8.4f} {m['wer']:>8.4f}")

print("-" * 120)
m = overall_metrics
print(f"{'OVERALL':<30} {m['count']:>8} {m['acc']*100:>7.2f}% {m['acc_ci']*100:>7.2f}% {m['acc_letters']*100:>7.2f}% {m['cer']:>8.4f} {m['cer_ci']:>8.4f} {m['cer_letters']:>8.4f} {m['wer']:>8.4f}")
print("="*120)

print("\n–õ–µ–≥–µ–Ω–¥–∞:")
print("  Acc      - Accuracy (case-sensitive)")
print("  Acc-CI   - Accuracy (case-insensitive)")
print("  Acc-L    - Accuracy (letters only, case-insensitive)")
print("  CER      - Character Error Rate")
print("  CER-CI   - CER (case-insensitive)")
print("  CER-L    - CER (letters only)")
print("  WER      - Word Error Rate")

print("\n=== –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è ===")
print(f"Case-only errors:                        {case_only_errors} ({case_only_errors/max(len(refs), 1)*100:.2f}%)")
print(f"Processed {len(images)} images in {total_time:.3f} sec")
print(f"Average per image: {avg_time:.3f} sec ({fps:.1f} FPS)")

# ============================================
# –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
# ============================================

def analyze_character_errors(refs, hyps):
    """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    
    substitutions = Counter()  # (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –æ—à–∏–±–æ—á–Ω—ã–π)
    insertions = Counter()     # –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    deletions = Counter()      # —É–¥–∞–ª—ë–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    
    error_positions = {'start': 0, 'middle': 0, 'end': 0}  # –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫
    
    for ref, hyp in zip(refs, hyps):
        if ref == hyp:
            continue
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ Levenshtein –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        ops = Levenshtein.editops(ref, hyp)
        
        for op_type, ref_pos, hyp_pos in ops:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –æ—à–∏–±–∫–∏ –≤ —Å–ª–æ–≤–µ
            word_len = len(ref)
            if ref_pos < word_len * 0.2:
                error_positions['start'] += 1
            elif ref_pos > word_len * 0.8:
                error_positions['end'] += 1
            else:
                error_positions['middle'] += 1
            
            if op_type == 'replace':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                substitutions[(ref_char, hyp_char)] += 1
            elif op_type == 'insert':
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                insertions[hyp_char] += 1
            elif op_type == 'delete':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                deletions[ref_char] += 1
    
    return substitutions, insertions, deletions, error_positions


def analyze_word_lengths(error_details):
    """–ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏"""
    error_lengths = []
    correct_lengths = []
    
    for detail in error_details:
        error_lengths.append(len(detail['ref']))
    
    return error_lengths


def analyze_error_types(error_details):
    """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫"""
    
    total_errors = len(error_details)
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—à–∏–±–æ–∫
    length_mismatch = 0
    case_errors = 0
    similar_chars = 0
    completely_wrong = 0
    
    for detail in error_details:
        ref = detail['ref']
        hyp = detail['hyp']
        
        if len(ref) != len(hyp):
            length_mismatch += 1
        elif ref.lower() == hyp.lower():
            case_errors += 1
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            distance = Levenshtein.distance(ref, hyp)
            if distance <= 2:
                similar_chars += 1
            else:
                completely_wrong += 1
    
    return {
        'total': total_errors,
        'length_mismatch': length_mismatch,
        'case_errors': case_errors,
        'similar_chars': similar_chars,
        'completely_wrong': completely_wrong
    }


if error_details:
    print("\n" + "="*80)
    print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö")
    print("="*80)
    
    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
    print(f"\n1Ô∏è‚É£ –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(refs)}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(refs) - len(error_details)}")
    print(f"   –° –æ—à–∏–±–∫–∞–º–∏: {len(error_details)} ({len(error_details)/len(refs)*100:.1f}%)")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫ –ø–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º
    print(f"\n   –ü–æ –¥–∞—Ç–∞—Å–µ—Ç–∞–º:")
    for idx in sorted(dataset_metrics.keys()):
        dataset = datasets[idx - 1]
        dataset_name = os.path.basename(dataset["image_dir"])
        dataset_errors = [e for e in error_details if e['dataset_id'] == idx]
        dataset_total = dataset_metrics[idx]['count']
        print(f"      {dataset_name}: {len(dataset_errors)}/{dataset_total} –æ—à–∏–±–æ–∫ ({len(dataset_errors)/max(dataset_total, 1)*100:.1f}%)")
    
    # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_cer_ci = 0.0
    total_wer_ci = 0.0
    total_cer_letters = 0.0
    for ref, hyp in zip(refs, hyps):
        total_cer_ci += character_error_rate(ref.lower(), hyp.lower())
        total_wer_ci += word_error_rate(ref.lower(), hyp.lower())
        
        # –ú–µ—Ç—Ä–∏–∫–∏ —Ç–æ–ª—å–∫–æ –ø–æ –±—É–∫–≤–∞–º
        ref_letters = normalize_text_letters_only(ref)
        hyp_letters = normalize_text_letters_only(hyp)
        if ref_letters:  # –ò–∑–±–µ–≥–∞–µ–º –¥–µ–ª–µ–Ω–∏—è –Ω–∞ –Ω–æ–ª—å –¥–ª—è –ø—É—Å—Ç—ã—Ö —Å—Ç—Ä–æ–∫
            total_cer_letters += character_error_rate(ref_letters, hyp_letters)
    
    avg_cer_ci = total_cer_ci / max(len(refs), 1)
    avg_wer_ci = total_wer_ci / max(len(refs), 1)
    avg_cer_letters = total_cer_letters / max(len(refs), 1)
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-sensitive):")
    print(f"      Accuracy: {acc*100:.2f}%")
    print(f"      CER: {avg_cer:.4f}")
    print(f"      WER: {avg_wer:.4f}")
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-insensitive):")
    print(f"      Accuracy: {acc_case_insensitive*100:.2f}%")
    print(f"      CER: {avg_cer_ci:.4f}")
    print(f"      WER: {avg_wer_ci:.4f}")
    print(f"      –£–ª—É—á—à–µ–Ω–∏–µ CER: {(avg_cer - avg_cer_ci)/avg_cer*100:.1f}%")
    print(f"      –£–ª—É—á—à–µ–Ω–∏–µ WER: {(avg_wer - avg_wer_ci)/avg_wer*100:.1f}%")
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (letters only, case-insensitive):")
    print(f"      Accuracy: {acc_letters_only*100:.2f}%")
    print(f"      CER: {avg_cer_letters:.4f}")
    print(f"      –£–ª—É—á—à–µ–Ω–∏–µ CER: {(avg_cer - avg_cer_letters)/avg_cer*100:.1f}%")
    
    # 2. –¢–∏–ø—ã –æ—à–∏–±–æ–∫
    print(f"\n2Ô∏è‚É£ –¢–ò–ü–´ –û–®–ò–ë–û–ö:")
    error_types = analyze_error_types(error_details)
    print(f"   –†–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞: {error_types['length_mismatch']} ({error_types['length_mismatch']/error_types['total']*100:.1f}%)")
    print(f"   –¢–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä: {error_types['case_errors']} ({error_types['case_errors']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ—Ö–æ–∂–∏–µ (1-2 —Å–∏–º–≤–æ–ª–∞): {error_types['similar_chars']} ({error_types['similar_chars']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–µ—Ä–Ω—ã–µ: {error_types['completely_wrong']} ({error_types['completely_wrong']/error_types['total']*100:.1f}%)")
    
    # 3. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    print(f"\n3Ô∏è‚É£ –î–õ–ò–ù–ê –°–õ–û–í –° –û–®–ò–ë–ö–ê–ú–ò:")
    error_lengths = analyze_word_lengths(error_details)
    if error_lengths:
        avg_error_len = sum(error_lengths) / len(error_lengths)
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_error_len:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∏–Ω: {min(error_lengths)}, –ú–∞–∫—Å: {max(error_lengths)}")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º
        length_dist = Counter(error_lengths)
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for length in sorted(length_dist.keys())[:10]:  # –¢–æ–ø-10
            print(f"      {length} —Å–∏–º–≤–æ–ª–æ–≤: {length_dist[length]} —Å–ª–æ–≤")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    print(f"\n4Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ü–û –°–ò–ú–í–û–õ–ê–ú:")
    substitutions, insertions, deletions, error_positions = analyze_character_errors(refs, hyps)
    
    # –ü–æ–∑–∏—Ü–∏–∏ –æ—à–∏–±–æ–∫
    total_pos = sum(error_positions.values())
    if total_pos > 0:
        print(f"   –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ —Å–ª–æ–≤–µ:")
        print(f"      –ù–∞—á–∞–ª–æ (0-20%): {error_positions['start']} ({error_positions['start']/total_pos*100:.1f}%)")
        print(f"      –°–µ—Ä–µ–¥–∏–Ω–∞ (20-80%): {error_positions['middle']} ({error_positions['middle']/total_pos*100:.1f}%)")
        print(f"      –ö–æ–Ω–µ—Ü (80-100%): {error_positions['end']} ({error_positions['end']/total_pos*100:.1f}%)")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã
    print(f"\n   üîÑ –¢–æ–ø-20 –∑–∞–º–µ–Ω —Å–∏–º–≤–æ–ª–æ–≤ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ‚Üí –æ—à–∏–±–æ—á–Ω—ã–π):")
    
    # –†–∞–∑–¥–µ–ª–∏–º –∑–∞–º–µ–Ω—ã –Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∏ –Ω–µ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ
    case_substitutions = []
    non_case_substitutions = []
    
    for (correct, wrong), count in substitutions.items():
        if correct.lower() == wrong.lower() and correct != wrong:
            case_substitutions.append(((correct, wrong), count))
        else:
            non_case_substitutions.append(((correct, wrong), count))
    
    case_substitutions.sort(key=lambda x: x[1], reverse=True)
    non_case_substitutions.sort(key=lambda x: x[1], reverse=True)
    
    if case_substitutions:
        print(f"\n      –†–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-10):")
        for (correct, wrong), count in case_substitutions[:10]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    if non_case_substitutions:
        print(f"\n      –î—Ä—É–≥–∏–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-20):")
        for (correct, wrong), count in non_case_substitutions[:20]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤—Å—Ç–∞–≤–∫–∏
    if insertions:
        print(f"\n   ‚ûï –¢–æ–ø-10 –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in insertions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —É–¥–∞–ª–µ–Ω–∏—è
    if deletions:
        print(f"\n   ‚ûñ –¢–æ–ø-10 –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in deletions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # 5. –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\n5Ô∏è‚É£ –•–£–î–®–ò–ï –ü–†–ò–ú–ï–†–´ (—Ç–æ–ø-10 –ø–æ CER):")
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:10]
    for i, ex in enumerate(worst_examples, 1):
        print(f"   {i}. [{ex['fname']}]")
        print(f"      GT:   '{ex['ref']}'")
        print(f"      Pred: '{ex['hyp']}'")
        print(f"      CER: {ex['cer']:.3f}, Conf: {ex['confidence']:.3f}")

    # === 5. –í–°–ï –û–®–ò–ë–ö–ò (—Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ 4 HTML, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω—ã –ø–æ GT) ===
    print(f"\n5Ô∏è‚É£ –°–û–ó–î–ê–Å–ú HTML-–û–¢–ß–Å–¢–´ –°–û –í–°–ï–ú–ò –û–®–ò–ë–ö–ê–ú–ò (—Ä–∞–∑–±–∏—Ç—ã–µ –Ω–∞ 4 —á–∞—Å—Ç–∏, —Å–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ GT)...")

    import base64
    from io import BytesIO
    from PIL import Image
    import math

    # === 1. –ë–µ—Ä—ë–º –≤—Å–µ –æ—à–∏–±–∫–∏, —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ GT ===
    all_errors = sorted(error_details, key=lambda x: x['ref'].lower())
    num_errors = len(all_errors)
    num_parts = 4
    part_size = math.ceil(num_errors / num_parts)

    print(f"   –í—Å–µ–≥–æ –æ—à–∏–±–æ–∫: {num_errors}")
    print(f"   –ë—É–¥–µ—Ç —Å–æ–∑–¥–∞–Ω–æ {num_parts} HTML-—Ñ–∞–π–ª–∞ –ø–æ ~{part_size} –∑–∞–ø–∏—Å–µ–π –∫–∞–∂–¥—ã–π")

    # === 2. –û–±—â–∏–π —Å—Ç–∏–ª—å –∏ JS (–µ–¥–∏–Ω—ã–µ –¥–ª—è –≤—Å–µ—Ö —á–∞—Å—Ç–µ–π) ===
    def make_html_header(title):
        return [
            "<html><head><meta charset='utf-8'>",
            "<style>",
            "body { font-family: Arial, sans-serif; background: #fafafa; }",
            "table { border-collapse: collapse; width: 100%; margin: 20px 0; table-layout: fixed; }",
            "th, td { border: 1px solid #ccc; padding: 6px 10px; text-align: left; vertical-align: middle; overflow-wrap: break-word; }",
            "th { background-color: #f2f2f2; }",
            "td:nth-child(2) { width: 150px; text-align: center; }",
            "img { max-width: 140px; max-height: 80px; object-fit: contain; border-radius: 6px; background: #fff; }",
            ".gt { color: #006400; font-weight: bold; }",
            ".pred { color: #8B0000; font-weight: bold; }",
            ".edit { background: #ffffe0; }",
            ".num { text-align: center; }",
            "button { margin: 10px; padding: 6px 10px; }",
            "</style></head><body>",
            f"<h2>{title}</h2>",
            "<div>",
            "<button onclick='resizeImages(0.5)'>üîç –£–º–µ–Ω—å—à–∏—Ç—å</button>",
            "<button onclick='resizeImages(1)'>üîé –ù–æ—Ä–º–∞–ª—å–Ω–æ</button>",
            "<button onclick='resizeImages(2)'>üîç –£–≤–µ–ª–∏—á–∏—Ç—å</button>",
            "<button onclick='downloadCorrections()'>üíæ –°–∫–∞—á–∞—Ç—å –ø—Ä–∞–≤–∫–∏ (CSV)</button>",
            "</div>",
            "<script>",
            "function resizeImages(scale){document.querySelectorAll('img').forEach(img=>{img.style.maxWidth=(140*scale)+'px';img.style.maxHeight=(80*scale)+'px';});}",
            "function saveCorrection(id){const val=document.getElementById('edit_'+id).innerText.trim();localStorage.setItem('ocr_edit_'+id,val);}",
            "function loadCorrections(){document.querySelectorAll('[id^=edit_]').forEach(el=>{const saved=localStorage.getItem('ocr_edit_'+el.id.split('edit_')[1]);if(saved){el.innerText=saved;}});}",
            "function downloadCorrections(){let rows=[['#','filename','GT','Pred','CER','Conf','Correction']];document.querySelectorAll('tr[data-id]').forEach(tr=>{const id=tr.getAttribute('data-id');const cells=tr.querySelectorAll('td');const correction=document.getElementById('edit_'+id).innerText.trim().replace(/\\n/g,' ');rows.push([id,cells[2].innerText,cells[3].innerText,cells[4].innerText,cells[5].innerText,cells[6].innerText,correction]);});const csvContent=rows.map(r=>r.map(v=>'\"'+v.replaceAll('\"','\"\"')+'\"').join(',')).join('\\n');const blob=new Blob([csvContent],{type:'text/csv;charset=utf-8;'});const a=document.createElement('a');a.href=URL.createObjectURL(blob);a.download='ocr_corrections.csv';a.click();}",
            "window.onload=loadCorrections;",
            "</script>",
            "<table>",
            "<tr><th>#</th><th>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</th><th>–§–∞–π–ª</th><th>GT</th><th>Pred</th><th>CER</th><th>Conf.</th><th>–ü—Ä–∞–≤–∫–∞ ‚úèÔ∏è</th></tr>"
        ]

    def make_html_row(i, ex):
        fname = ex['fname']
        cer = f"{ex['cer']:.3f}"
        conf = f"{ex['confidence']:.3f}"
        gt = ex['ref'].replace("<", "&lt;").replace(">", "&gt;")
        pred = ex['hyp'].replace("<", "&lt;").replace(">", "&gt;")

        # –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        img_path = None
        for d in datasets:
            candidate = os.path.join(d["image_dir"], fname)
            if os.path.exists(candidate):
                img_path = candidate
                break

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        if img_path:
            try:
                with Image.open(img_path) as img:
                    img.thumbnail((400, 200))
                    buffer = BytesIO()
                    img.save(buffer, format="JPEG", quality=80)
                    img_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
                    img_tag = f"<img src='data:image/jpeg;base64,{img_base64}'>"
            except Exception:
                img_tag = f"<div style='color:red;'>–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏</div>"
        else:
            img_tag = "<div style='color:gray;'>–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</div>"

        return (
            f"<tr data-id='{i}'>"
            f"<td class='num'>{i}</td>"
            f"<td>{img_tag}</td>"
            f"<td>{fname}</td>"
            f"<td class='gt'>{gt}</td>"
            f"<td class='pred'>{pred}</td>"
            f"<td class='num'>{cer}</td>"
            f"<td class='num'>{conf}</td>"
            f"<td class='edit' id='edit_{i}' contenteditable='true' oninput='saveCorrection({i})'></td>"
            f"</tr>"
        )

    # === 3. –ì–µ–Ω–µ—Ä–∞—Ü–∏—è 4 HTML-—Ñ–∞–π–ª–æ–≤ ===
    for part_idx in range(num_parts):
        start = part_idx * part_size
        end = min(start + part_size, num_errors)
        subset = all_errors[start:end]

        if not subset:
            continue

        html_lines = make_html_header(f"üìä OCR –æ—à–∏–±–∫–∏ (—á–∞—Å—Ç—å {part_idx+1} –∏–∑ {num_parts}) ‚Äî –∑–∞–ø–∏—Å–∏ {start+1}‚Äì{end}")
        for i, ex in enumerate(subset, start + 1):
            html_lines.append(make_html_row(i, ex))
        html_lines.append("</table></body></html>")

        html_path = os.path.join(
            os.path.dirname(model_path),
            f"ocr_all_errors_part{part_idx+1}.html"
        )

        with open(html_path, "w", encoding="utf-8") as f:
            f.write("\n".join(html_lines))

        print(f"üíæ HTML-–æ—Ç—á—ë—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_path}")

    print(f"\n‚úÖ –°–æ–∑–¥–∞–Ω–æ {num_parts} HTML-—Ñ–∞–π–ª–æ–≤ —Å–æ –≤—Å–µ–º–∏ {num_errors} –æ—à–∏–±–∫–∞–º–∏ (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ –ø–æ GT).")


    # 6. –°–≤—è–∑—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–æ–∫
    print(f"\n6Ô∏è‚É£ –°–í–Ø–ó–¨ –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ò –û–®–ò–ë–û–ö:")
    low_conf_errors = [e for e in error_details if e['confidence'] < 0.8]
    high_conf_errors = [e for e in error_details if e['confidence'] >= 0.8]
    print(f"   –û—à–∏–±–∫–∏ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (<0.8): {len(low_conf_errors)} ({len(low_conf_errors)/len(error_details)*100:.1f}%)")
    print(f"   –û—à–∏–±–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (‚â•0.8): {len(high_conf_errors)} ({len(high_conf_errors)/len(error_details)*100:.1f}%)")
    
    if error_details:
        avg_conf_errors = sum(e['confidence'] for e in error_details) / len(error_details)
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—à–∏–±–∫–∞—Ö: {avg_conf_errors:.3f}")
   
    # 7. –í—Å–µ –æ—à–∏–±–∫–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    print(f"\n7Ô∏è‚É£ –í–°–ï –û–®–ò–ë–ö–ò (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏):")
    sorted_errors = sorted(error_details, key=lambda x: x['confidence'], reverse=True)
    
    print(f"{'–§–∞–π–ª':30s} | {'Conf.':>7s} | {'CER':>5s} | {'GT':25s} | {'Pred':25s}")
    print("-" * 100)
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ CSV ===
    import csv

    output_csv = os.path.join(os.path.dirname(model_path), "ocr_errors_by_confidence.csv")
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["filename", "confidence", "CER", "WER", "GT", "Prediction"])
        for err in sorted_errors:
            writer.writerow([
                err['fname'],
                f"{err['confidence']:.4f}",
                f"{err['cer']:.4f}",
                f"{err['wer']:.4f}",
                err['ref'],
                err['hyp'],
            ])

    print(f"\nüíæ –û—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_csv}")
else:
    print("\n‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫! –í—Å–µ —Å–ª–æ–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã –∏–¥–µ–∞–ª—å–Ω–æ!")

print("\n" + "="*80)