import os
import time
import csv
from collections import Counter, defaultdict
from manuscript.recognizers import TRBA
from manuscript.recognizers._trba.training.metrics import (
    character_error_rate,
    word_error_rate,
    compute_accuracy,
)
import Levenshtein


# === –ü—É—Ç–∏ ===
# –ú–æ–∂–Ω–æ —É–∫–∞–∑–∞—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ - –æ–Ω–∏ –±—É–¥—É—Ç –æ–±—ä–µ–¥–∏–Ω–µ–Ω—ã
datasets = [
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\img",
        "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\printed\val\labels.csv",
    },
    # –†–∞—Å–∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ –¥–ª—è –¥–æ–±–∞–≤–ª–µ–Ω–∏—è –≤—Ç–æ—Ä–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞:
    {
        "image_dir": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\img",
         "gt_path": r"C:\Users\USER\Desktop\archive_25_09\dataset\handwritten\val\labels.csv",
    },
]

model_path = r"C:\Users\USER\Desktop\OCR_MODELS\exp_2\best_acc_weights.pth"
config_path = r"C:\Users\USER\Desktop\OCR_MODELS\exp_2\config.json"

batch_size = 64

# === –ß–∏—Ç–∞–µ–º GT-—Ñ–∞–π–ª—ã –∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ===
gt_data = {}
total_gt_lines = 0

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    gt_path = dataset["gt_path"]
    
    print(f"üìÇ –î–∞—Ç–∞—Å–µ—Ç {idx}: {os.path.basename(image_dir)}")
    
    dataset_gt = {}
    with open(gt_path, "r", encoding="utf-8") as f:
        reader = csv.reader(f)
        for row in reader:
            if len(row) >= 2:
                fname = row[0].strip()
                text = ",".join(row[1:]).strip()  # –ù–∞ —Å–ª—É—á–∞–π –µ—Å–ª–∏ –≤ —Ç–µ–∫—Å—Ç–µ –µ—Å—Ç—å –∑–∞–ø—è—Ç—ã–µ
                dataset_gt[fname] = text
    
    print(f"   –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(dataset_gt)} –∑–∞–ø–∏—Å–µ–π –∏–∑ {os.path.basename(gt_path)}")
    total_gt_lines += len(dataset_gt)
    
    # –î–æ–±–∞–≤–ª—è–µ–º –≤ –æ–±—â–∏–π —Å–ª–æ–≤–∞—Ä—å —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    for fname, text in dataset_gt.items():
        if fname in gt_data:
            print(f"   ‚ö†Ô∏è  –î—É–±–ª–∏–∫–∞—Ç —Ñ–∞–π–ª–∞: {fname} (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∞ –ø–æ—Å–ª–µ–¥–Ω—è—è –≤–µ—Ä—Å–∏—è)")
        gt_data[fname] = text

print(f"\nüìÑ –í—Å–µ–≥–æ –∑–∞–≥—Ä—É–∂–µ–Ω–æ {total_gt_lines} –∑–∞–ø–∏—Å–µ–π –∏–∑ {len(datasets)} –¥–∞—Ç–∞—Å–µ—Ç–æ–≤(–∞)")
print(f"üìÑ –£–Ω–∏–∫–∞–ª—å–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤: {len(gt_data)}")

# === –°–∫–∞–Ω–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –≤—Å–µ—Ö –¥–∞—Ç–∞—Å–µ—Ç–æ–≤ ===
valid_ext = {".png", ".jpg", ".jpeg", ".tif", ".tiff", ".bmp"}
images = []

for idx, dataset in enumerate(datasets, 1):
    image_dir = dataset["image_dir"]
    
    dataset_images = [
        os.path.join(image_dir, f)
        for f in os.listdir(image_dir)
        if os.path.splitext(f)[1].lower() in valid_ext
    ]
    
    if not dataset_images:
        print(f"‚ö†Ô∏è  –î–∞—Ç–∞—Å–µ—Ç {idx}: –í –ø–∞–ø–∫–µ {image_dir} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π!")
    else:
        print(f"üìÅ –î–∞—Ç–∞—Å–µ—Ç {idx}: –ù–∞–π–¥–µ–Ω–æ {len(dataset_images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
        images.extend(dataset_images)

if not images:
    raise RuntimeError(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∏ –≤ –æ–¥–Ω–æ–º –¥–∞—Ç–∞—Å–µ—Ç–µ!")

print(f"\nüìÅ –ò–¢–û–ì–û: {len(images)} –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è")

# === –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏ ===
recognizer = TRBA(model_path=model_path, config_path=config_path)

# === –í—ã–±–æ—Ä —Ä–µ–∂–∏–º–∞ –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è ===
# –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ä–µ–∂–∏–º—ã: "greedy", "beam"
decode_mode = "greedy"  # –ò–∑–º–µ–Ω–∏—Ç–µ –Ω–∞ "beam" –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
print(f"üîß –†–µ–∂–∏–º –¥–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {decode_mode}")

# === –†–∞—Å–ø–æ–∑–Ω–∞—ë–º ===
start_time = time.perf_counter()
results = recognizer.predict(images=images, batch_size=batch_size, mode=decode_mode)
end_time = time.perf_counter()
print(results)
total_time = end_time - start_time
avg_time = total_time / len(images)
fps = 1.0 / avg_time if avg_time > 0 else float("inf")

# === –°–æ–ø–æ—Å—Ç–∞–≤–ª—è–µ–º —Å ground truth ===
refs, hyps = [], []
total_cer, total_wer = 0.0, 0.0
cer_count, wer_count = 0, 0
error_details = []  # –î–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞ –æ—à–∏–±–æ–∫

print("\n=== –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è ===")
for path, result in zip(images, results):
    pred_text = result["text"]
    score = result["confidence"]
    fname = os.path.basename(path)
    ref_text = gt_data.get(fname)
    if ref_text is None:
        print(f"{fname:40s} ‚Üí {pred_text:20s} (no GT)")
        continue

    refs.append(ref_text)
    hyps.append(pred_text)

    cer = character_error_rate(ref_text, pred_text)
    wer = word_error_rate(ref_text, pred_text)

    total_cer += cer
    total_wer += wer
    cer_count += 1
    wer_count += 1
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –¥–µ—Ç–∞–ª–∏ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
    if ref_text != pred_text:
        error_details.append({
            'fname': fname,
            'ref': ref_text,
            'hyp': pred_text,
            'cer': cer,
            'wer': wer,
            'confidence': score
        })

    print(
        f"{fname:40s} ‚Üí {pred_text:20s} | GT: {ref_text:20s} | CER={cer:.3f} | WER={wer:.3f}"
    )

# === –ú–µ—Ç—Ä–∏–∫–∏ ===
acc = compute_accuracy(refs, hyps)

# –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å (case-insensitive)
acc_case_insensitive = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower()) / max(len(refs), 1)

# –¢–æ—á–Ω–æ—Å—Ç—å —Å —É—á–µ—Ç–æ–º —Ç–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä–∞ (–∫–æ–≥–¥–∞ –≤—Å–µ —Å–∏–º–≤–æ–ª—ã –≤–µ—Ä–Ω—ã, –Ω–æ —Ä–µ–≥–∏—Å—Ç—Ä –¥—Ä—É–≥–æ–π)
case_only_errors = sum(1 for r, h in zip(refs, hyps) if r.lower() == h.lower() and r != h)

avg_cer = total_cer / max(cer_count, 1)
avg_wer = total_wer / max(wer_count, 1)

print("\n=== –°–≤–æ–¥–∫–∞ ===")
print(f"Accuracy (case-sensitive):     {acc*100:.2f}%")
print(f"Accuracy (case-insensitive):   {acc_case_insensitive*100:.2f}%")
print(f"Case-only errors:              {case_only_errors} ({case_only_errors/max(len(refs), 1)*100:.2f}%)")
print(f"Avg CER:  {avg_cer:.4f}")
print(f"Avg WER:  {avg_wer:.4f}")
print(f"Processed {len(images)} images in {total_time:.3f} sec")
print(f"Average per image: {avg_time:.3f} sec ({fps:.1f} FPS)")

# ============================================
# –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö
# ============================================

def analyze_character_errors(refs, hyps):
    """–ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å–∏–º–≤–æ–ª–æ–≤"""
    
    substitutions = Counter()  # (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π, –æ—à–∏–±–æ—á–Ω—ã–π)
    insertions = Counter()     # –≤—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    deletions = Counter()      # —É–¥–∞–ª—ë–Ω–Ω—ã–π —Å–∏–º–≤–æ–ª
    
    error_positions = {'start': 0, 'middle': 0, 'end': 0}  # –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫
    
    for ref, hyp in zip(refs, hyps):
        if ref == hyp:
            continue
            
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–ø–µ—Ä–∞—Ü–∏–∏ Levenshtein –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞
        ops = Levenshtein.editops(ref, hyp)
        
        for op_type, ref_pos, hyp_pos in ops:
            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–æ–∑–∏—Ü–∏—é –æ—à–∏–±–∫–∏ –≤ —Å–ª–æ–≤–µ
            word_len = len(ref)
            if ref_pos < word_len * 0.2:
                error_positions['start'] += 1
            elif ref_pos > word_len * 0.8:
                error_positions['end'] += 1
            else:
                error_positions['middle'] += 1
            
            if op_type == 'replace':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                substitutions[(ref_char, hyp_char)] += 1
            elif op_type == 'insert':
                hyp_char = hyp[hyp_pos] if hyp_pos < len(hyp) else ''
                insertions[hyp_char] += 1
            elif op_type == 'delete':
                ref_char = ref[ref_pos] if ref_pos < len(ref) else ''
                deletions[ref_char] += 1
    
    return substitutions, insertions, deletions, error_positions


def analyze_word_lengths(error_details):
    """–ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏"""
    error_lengths = []
    correct_lengths = []
    
    for detail in error_details:
        error_lengths.append(len(detail['ref']))
    
    return error_lengths


def analyze_error_types(error_details):
    """–ê–Ω–∞–ª–∏–∑ —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫"""
    
    total_errors = len(error_details)
    
    # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –æ—à–∏–±–æ–∫
    length_mismatch = 0
    case_errors = 0
    similar_chars = 0
    completely_wrong = 0
    
    for detail in error_details:
        ref = detail['ref']
        hyp = detail['hyp']
        
        if len(ref) != len(hyp):
            length_mismatch += 1
        elif ref.lower() == hyp.lower():
            case_errors += 1
        else:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ö–æ–∂–µ—Å—Ç—å
            distance = Levenshtein.distance(ref, hyp)
            if distance <= 2:
                similar_chars += 1
            else:
                completely_wrong += 1
    
    return {
        'total': total_errors,
        'length_mismatch': length_mismatch,
        'case_errors': case_errors,
        'similar_chars': similar_chars,
        'completely_wrong': completely_wrong
    }


if error_details:
    print("\n" + "="*80)
    print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö")
    print("="*80)
    
    # 1. –û–±—â–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—à–∏–±–æ–∫
    print(f"\n1Ô∏è‚É£ –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
    print(f"   –í—Å–µ–≥–æ –ø—Ä–∏–º–µ—Ä–æ–≤: {len(refs)}")
    print(f"   –ü—Ä–∞–≤–∏–ª—å–Ω—ã—Ö: {len(refs) - len(error_details)}")
    print(f"   –° –æ—à–∏–±–∫–∞–º–∏: {len(error_details)} ({len(error_details)/len(refs)*100:.1f}%)")
    
    # –†–µ–≥–∏—Å—Ç—Ä–æ–Ω–µ–∑–∞–≤–∏—Å–∏–º—ã–µ –º–µ—Ç—Ä–∏–∫–∏
    total_cer_ci = 0.0
    total_wer_ci = 0.0
    for ref, hyp in zip(refs, hyps):
        total_cer_ci += character_error_rate(ref.lower(), hyp.lower())
        total_wer_ci += word_error_rate(ref.lower(), hyp.lower())
    
    avg_cer_ci = total_cer_ci / max(len(refs), 1)
    avg_wer_ci = total_wer_ci / max(len(refs), 1)
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-sensitive):")
    print(f"      Accuracy: {acc*100:.2f}%")
    print(f"      CER: {avg_cer:.4f}")
    print(f"      WER: {avg_wer:.4f}")
    
    print(f"\n   üìè –ú–µ—Ç—Ä–∏–∫–∏ (case-insensitive):")
    print(f"      Accuracy: {acc_case_insensitive*100:.2f}%")
    print(f"      CER: {avg_cer_ci:.4f}")
    print(f"      WER: {avg_wer_ci:.4f}")
    print(f"      –£–ª—É—á—à–µ–Ω–∏–µ CER: {(avg_cer - avg_cer_ci)/avg_cer*100:.1f}%")
    print(f"      –£–ª—É—á—à–µ–Ω–∏–µ WER: {(avg_wer - avg_wer_ci)/avg_wer*100:.1f}%")
    
    # 2. –¢–∏–ø—ã –æ—à–∏–±–æ–∫
    print(f"\n2Ô∏è‚É£ –¢–ò–ü–´ –û–®–ò–ë–û–ö:")
    error_types = analyze_error_types(error_details)
    print(f"   –†–∞–∑–Ω–∞—è –¥–ª–∏–Ω–∞: {error_types['length_mismatch']} ({error_types['length_mismatch']/error_types['total']*100:.1f}%)")
    print(f"   –¢–æ–ª—å–∫–æ —Ä–µ–≥–∏—Å—Ç—Ä: {error_types['case_errors']} ({error_types['case_errors']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ—Ö–æ–∂–∏–µ (1-2 —Å–∏–º–≤–æ–ª–∞): {error_types['similar_chars']} ({error_types['similar_chars']/error_types['total']*100:.1f}%)")
    print(f"   –ü–æ–ª–Ω–æ—Å—Ç—å—é –Ω–µ–≤–µ—Ä–Ω—ã–µ: {error_types['completely_wrong']} ({error_types['completely_wrong']/error_types['total']*100:.1f}%)")
    
    # 3. –ê–Ω–∞–ª–∏–∑ –¥–ª–∏–Ω—ã —Å–ª–æ–≤ —Å –æ—à–∏–±–∫–∞–º–∏
    print(f"\n3Ô∏è‚É£ –î–õ–ò–ù–ê –°–õ–û–í –° –û–®–ò–ë–ö–ê–ú–ò:")
    error_lengths = analyze_word_lengths(error_details)
    if error_lengths:
        avg_error_len = sum(error_lengths) / len(error_lengths)
        print(f"   –°—Ä–µ–¥–Ω—è—è –¥–ª–∏–Ω–∞: {avg_error_len:.1f} —Å–∏–º–≤–æ–ª–æ–≤")
        print(f"   –ú–∏–Ω: {min(error_lengths)}, –ú–∞–∫—Å: {max(error_lengths)}")
        
        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–ª–∏–Ω–∞–º
        length_dist = Counter(error_lengths)
        print(f"   –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ:")
        for length in sorted(length_dist.keys())[:10]:  # –¢–æ–ø-10
            print(f"      {length} —Å–∏–º–≤–æ–ª–æ–≤: {length_dist[length]} —Å–ª–æ–≤")
    
    # 4. –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫ –ø–æ —Å–∏–º–≤–æ–ª–∞–º
    print(f"\n4Ô∏è‚É£ –ê–ù–ê–õ–ò–ó –û–®–ò–ë–û–ö –ü–û –°–ò–ú–í–û–õ–ê–ú:")
    substitutions, insertions, deletions, error_positions = analyze_character_errors(refs, hyps)
    
    # –ü–æ–∑–∏—Ü–∏–∏ –æ—à–∏–±–æ–∫
    total_pos = sum(error_positions.values())
    if total_pos > 0:
        print(f"   –ü–æ–∑–∏—Ü–∏—è –æ—à–∏–±–æ–∫ –≤ —Å–ª–æ–≤–µ:")
        print(f"      –ù–∞—á–∞–ª–æ (0-20%): {error_positions['start']} ({error_positions['start']/total_pos*100:.1f}%)")
        print(f"      –°–µ—Ä–µ–¥–∏–Ω–∞ (20-80%): {error_positions['middle']} ({error_positions['middle']/total_pos*100:.1f}%)")
        print(f"      –ö–æ–Ω–µ—Ü (80-100%): {error_positions['end']} ({error_positions['end']/total_pos*100:.1f}%)")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –∑–∞–º–µ–Ω—ã
    print(f"\n   üîÑ –¢–æ–ø-20 –∑–∞–º–µ–Ω —Å–∏–º–≤–æ–ª–æ–≤ (–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π ‚Üí –æ—à–∏–±–æ—á–Ω—ã–π):")
    
    # –†–∞–∑–¥–µ–ª–∏–º –∑–∞–º–µ–Ω—ã –Ω–∞ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∏ –Ω–µ —Ä–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ
    case_substitutions = []
    non_case_substitutions = []
    
    for (correct, wrong), count in substitutions.items():
        if correct.lower() == wrong.lower() and correct != wrong:
            case_substitutions.append(((correct, wrong), count))
        else:
            non_case_substitutions.append(((correct, wrong), count))
    
    case_substitutions.sort(key=lambda x: x[1], reverse=True)
    non_case_substitutions.sort(key=lambda x: x[1], reverse=True)
    
    if case_substitutions:
        print(f"\n      –†–µ–≥–∏—Å—Ç—Ä–æ–≤—ã–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-10):")
        for (correct, wrong), count in case_substitutions[:10]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    if non_case_substitutions:
        print(f"\n      –î—Ä—É–≥–∏–µ –∑–∞–º–µ–Ω—ã (—Ç–æ–ø-20):")
        for (correct, wrong), count in non_case_substitutions[:20]:
            print(f"         '{correct}' ‚Üí '{wrong}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ –≤—Å—Ç–∞–≤–∫–∏
    if insertions:
        print(f"\n   ‚ûï –¢–æ–ø-10 –ª–∏—à–Ω–∏—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in insertions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # –°–∞–º—ã–µ —á–∞—Å—Ç—ã–µ —É–¥–∞–ª–µ–Ω–∏—è
    if deletions:
        print(f"\n   ‚ûñ –¢–æ–ø-10 –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö —Å–∏–º–≤–æ–ª–æ–≤:")
        for char, count in deletions.most_common(10):
            print(f"      '{char}': {count} —Ä–∞–∑")
    
    # 5. –•—É–¥—à–∏–µ –ø—Ä–∏–º–µ—Ä—ã
    print(f"\n5Ô∏è‚É£ –•–£–î–®–ò–ï –ü–†–ò–ú–ï–†–´ (—Ç–æ–ø-10 –ø–æ CER):")
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:10]
    for i, ex in enumerate(worst_examples, 1):
        print(f"   {i}. [{ex['fname']}]")
        print(f"      GT:   '{ex['ref']}'")
        print(f"      Pred: '{ex['hyp']}'")
        print(f"      CER: {ex['cer']:.3f}, Conf: {ex['confidence']:.3f}")

    # 5. –¢–æ–ø-100 —Ö—É–¥—à–∏—Ö –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ CER —Å HTML-–æ—Ç—á–µ—Ç–æ–º (—Å–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏)
    print(f"\n5Ô∏è‚É£ –¢–û–ü-100 –•–£–î–®–ò–• –ü–†–ò–ú–ï–†–û–í (–ø–æ CER) ‚Äî HTML –û–¢–ß–Å–¢ (–≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è)")

    import base64
    from io import BytesIO
    from PIL import Image

    top_n = 1000
    worst_examples = sorted(error_details, key=lambda x: x['cer'], reverse=True)[:top_n]

    html_path = os.path.join(os.path.dirname(model_path), "ocr_top_100_errors_embedded.html")

    html = [
        "<html><head><meta charset='utf-8'>",
        "<style>",
        "body { font-family: Arial, sans-serif; background: #fafafa; }",
        "table { border-collapse: collapse; width: 100%; margin: 20px 0; table-layout: fixed; }",
        "th, td { border: 1px solid #ccc; padding: 6px 10px; text-align: left; vertical-align: middle; overflow-wrap: break-word; }",
        "th { background-color: #f2f2f2; }",
        "td:nth-child(2) { width: 150px; text-align: center; }",
        "img { max-width: 140px; max-height: 80px; object-fit: contain; border-radius: 6px; background: #fff; }",
        ".gt { color: #006400; font-weight: bold; }",
        ".pred { color: #8B0000; font-weight: bold; }",
        ".num { text-align: center; }",
        "button { margin: 10px; padding: 6px 10px; }",
        "</style></head><body>",
        f"<h2>üìä –¢–æ–ø-{top_n} —Ö—É–¥—à–∏—Ö –æ—à–∏–±–æ–∫ OCR (–ø–æ CER)</h2>",
        "<button onclick='resizeImages(0.5)'>üîç –£–º–µ–Ω—å—à–∏—Ç—å</button>",
        "<button onclick='resizeImages(1)'>üîé –ù–æ—Ä–º–∞–ª—å–Ω–æ</button>",
        "<button onclick='resizeImages(2)'>üîç –£–≤–µ–ª–∏—á–∏—Ç—å</button>",
        "<script>",
        "function resizeImages(scale){",
        "  document.querySelectorAll('img').forEach(img=>{",
        "    img.style.maxWidth = (140*scale)+'px';",
        "    img.style.maxHeight = (80*scale)+'px';",
        "  });",
        "}",
        "</script>",
        "<table>",
        "<tr><th>#</th><th>–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ</th><th>–§–∞–π–ª</th><th>GT</th><th>Pred</th><th>CER</th><th>Conf.</th></tr>"
    ]

    for i, ex in enumerate(worst_examples, 1):
        fname = ex['fname']
        cer = f"{ex['cer']:.3f}"
        conf = f"{ex['confidence']:.3f}"
        gt = ex['ref'].replace("<", "&lt;").replace(">", "&gt;")
        pred = ex['hyp'].replace("<", "&lt;").replace(">", "&gt;")

        # –ù–∞—Ö–æ–¥–∏–º –ø—É—Ç—å –∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—é
        img_path = None
        for d in datasets:
            candidate = os.path.join(d["image_dir"], fname)
            if os.path.exists(candidate):
                img_path = candidate
                break

        # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ base64
        if img_path:
            try:
                with Image.open(img_path) as img:
                    img.thumbnail((400, 200))  # —É–º–µ–Ω—å—à–∞–µ–º –¥–ª—è –∫–æ–º–ø–∞–∫—Ç–Ω–æ—Å—Ç–∏
                    buffer = BytesIO()
                    img.save(buffer, format="JPEG", quality=80)
                    img_base64 = base64.b64encode(buffer.getvalue()).decode("utf-8")
                    img_tag = f"<img src='data:image/jpeg;base64,{img_base64}'>"
            except Exception as e:
                img_tag = f"<div style='color:red;'>–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏</div>"
        else:
            img_tag = "<div style='color:gray;'>–ù–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è</div>"

        html.append(
            f"<tr>"
            f"<td class='num'>{i}</td>"
            f"<td>{img_tag}</td>"
            f"<td>{fname}</td>"
            f"<td class='gt'>{gt}</td>"
            f"<td class='pred'>{pred}</td>"
            f"<td class='num'>{cer}</td>"
            f"<td class='num'>{conf}</td>"
            f"</tr>"
        )

    html.append("</table></body></html>")

    with open(html_path, "w", encoding="utf-8") as f:
        f.write("\n".join(html))

    print(f"üíæ HTML-–æ—Ç—á—ë—Ç —Å–æ –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–º–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è–º–∏ —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {html_path}")

    # 6. –°–≤—è–∑—å —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –∏ –æ—à–∏–±–æ–∫
    print(f"\n6Ô∏è‚É£ –°–í–Ø–ó–¨ –£–í–ï–†–ï–ù–ù–û–°–¢–ò –ò –û–®–ò–ë–û–ö:")
    low_conf_errors = [e for e in error_details if e['confidence'] < 0.8]
    high_conf_errors = [e for e in error_details if e['confidence'] >= 0.8]
    print(f"   –û—à–∏–±–∫–∏ —Å –Ω–∏–∑–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (<0.8): {len(low_conf_errors)} ({len(low_conf_errors)/len(error_details)*100:.1f}%)")
    print(f"   –û—à–∏–±–∫–∏ —Å –≤—ã—Å–æ–∫–æ–π —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å—é (‚â•0.8): {len(high_conf_errors)} ({len(high_conf_errors)/len(error_details)*100:.1f}%)")
    
    if error_details:
        avg_conf_errors = sum(e['confidence'] for e in error_details) / len(error_details)
        print(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å –Ω–∞ –æ—à–∏–±–∫–∞—Ö: {avg_conf_errors:.3f}")
   
    # 7. –í—Å–µ –æ—à–∏–±–∫–∏, –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    print(f"\n7Ô∏è‚É£ –í–°–ï –û–®–ò–ë–ö–ò (–æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏):")
    sorted_errors = sorted(error_details, key=lambda x: x['confidence'], reverse=True)
    
    print(f"{'–§–∞–π–ª':30s} | {'Conf.':>7s} | {'CER':>5s} | {'GT':25s} | {'Pred':25s}")
    print("-" * 100)
    # === –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –æ—à–∏–±–æ–∫ –≤ CSV ===
    import csv

    output_csv = os.path.join(os.path.dirname(model_path), "ocr_errors_by_confidence.csv")
    with open(output_csv, "w", newline="", encoding="utf-8") as csvfile:
        writer = csv.writer(csvfile)
        writer.writerow(["filename", "confidence", "CER", "WER", "GT", "Prediction"])
        for err in sorted_errors:
            writer.writerow([
                err['fname'],
                f"{err['confidence']:.4f}",
                f"{err['cer']:.4f}",
                f"{err['wer']:.4f}",
                err['ref'],
                err['hyp'],
            ])

    print(f"\nüíæ –û—à–∏–±–∫–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ —Ñ–∞–π–ª: {output_csv}")
else:
    print("\n‚úÖ –ù–µ—Ç –æ—à–∏–±–æ–∫! –í—Å–µ —Å–ª–æ–≤–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–Ω—ã –∏–¥–µ–∞–ª—å–Ω–æ!")

print("\n" + "="*80)